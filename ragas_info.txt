Help on package ragas.metrics in ragas:

NAME
    ragas.metrics

PACKAGE CONTENTS
    _answer_correctness
    _answer_relevance
    _answer_similarity
    _aspect_critic
    _bleu_score
    _chrf_score
    _context_entities_recall
    _context_precision
    _context_recall
    _datacompy_score
    _domain_specific_rubrics
    _factual_correctness
    _faithfulness
    _goal_accuracy
    _instance_specific_rubrics
    _multi_modal_faithfulness
    _multi_modal_relevance
    _noise_sensitivity
    _nv_metrics
    _rouge_score
    _simple_criteria
    _sql_semantic_equivalence
    _string
    _summarization
    _tool_call_accuracy
    _tool_call_f1
    _topic_adherence
    base
    collections (package)
    decorator
    discrete
    numeric
    quoted_spans
    ranking
    result
    utils
    validators

CLASSES
    abc.ABC(builtins.object)
        ragas.metrics.base.Metric
            ragas.metrics.base.MetricWithEmbeddings
                ragas.metrics._answer_similarity.SemanticSimilarity(ragas.metrics.base.MetricWithEmbeddings, ragas.metrics.base.SingleTurnMetric)
                    ragas.metrics._answer_similarity.AnswerSimilarity
            ragas.metrics.base.MetricWithLLM(ragas.metrics.base.Metric, ragas.prompt.mixin.PromptMixin)
                ragas.metrics._answer_correctness.AnswerCorrectness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MetricWithEmbeddings, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._answer_relevance.ResponseRelevancy(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MetricWithEmbeddings, ragas.metrics.base.SingleTurnMetric)
                    ragas.metrics._answer_relevance.AnswerRelevancy
                ragas.metrics._aspect_critic.AspectCritic(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
                ragas.metrics._context_entities_recall.ContextEntityRecall(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._context_precision.LLMContextPrecisionWithReference(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                    ragas.metrics._context_precision.ContextPrecision
                    ragas.metrics._context_precision.LLMContextPrecisionWithoutReference
                        ragas.metrics._context_precision.ContextUtilization
                ragas.metrics._context_recall.LLMContextRecall(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                    ragas.metrics._context_recall.ContextRecall
                ragas.metrics._domain_specific_rubrics.RubricsScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
                ragas.metrics._factual_correctness.FactualCorrectness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._faithfulness.Faithfulness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                    ragas.metrics._faithfulness.FaithfulnesswithHHEM
                ragas.metrics._goal_accuracy.AgentGoalAccuracyWithReference(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MultiTurnMetric)
                ragas.metrics._goal_accuracy.AgentGoalAccuracyWithoutReference(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MultiTurnMetric)
                ragas.metrics._instance_specific_rubrics.InstanceRubrics(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
                ragas.metrics._multi_modal_faithfulness.MultiModalFaithfulness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._multi_modal_relevance.MultiModalRelevance(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._noise_sensitivity.NoiseSensitivity(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._nv_metrics.AnswerAccuracy(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._nv_metrics.ContextRelevance(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._nv_metrics.ResponseGroundedness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._simple_criteria.SimpleCriteriaScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
                ragas.metrics._sql_semantic_equivalence.LLMSQLEquivalence(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._summarization.SummarizationScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
                ragas.metrics._topic_adherence.TopicAdherenceScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MultiTurnMetric)
            ragas.metrics.base.MultiTurnMetric
                ragas.metrics._tool_call_accuracy.ToolCallAccuracy
                ragas.metrics._tool_call_f1.ToolCallF1
            ragas.metrics.base.SingleTurnMetric
                ragas.metrics._bleu_score.BleuScore
                ragas.metrics._chrf_score.ChrfScore
                ragas.metrics._context_precision.IDBasedContextPrecision
                ragas.metrics._context_precision.NonLLMContextPrecisionWithReference
                ragas.metrics._context_recall.IDBasedContextRecall
                ragas.metrics._context_recall.NonLLMContextRecall
                ragas.metrics._datacompy_score.DataCompyScore
                ragas.metrics._rouge_score.RougeScore
                ragas.metrics._string.ExactMatch
                ragas.metrics._string.NonLLMStringSimilarity
                ragas.metrics._string.StringPresence
        ragas.metrics.base.SimpleBaseMetric
            ragas.metrics.base.SimpleLLMMetric
                ragas.metrics.discrete.DiscreteMetric(ragas.metrics.base.SimpleLLMMetric, ragas.metrics.validators.DiscreteValidator)
                ragas.metrics.numeric.NumericMetric(ragas.metrics.base.SimpleLLMMetric, ragas.metrics.validators.NumericValidator)
                ragas.metrics.ranking.RankingMetric(ragas.metrics.base.SimpleLLMMetric, ragas.metrics.validators.RankingValidator)
    builtins.object
        ragas.metrics.result.MetricResult
    enum.Enum(builtins.object)
        ragas.metrics._string.DistanceMeasure
        ragas.metrics.base.MetricOutputType
        ragas.metrics.base.MetricType

    class AgentGoalAccuracyWithReference(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MultiTurnMetric)
     |  AgentGoalAccuracyWithReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'agent_goal_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, workflow_prompt: 'PydanticPrompt' = <factory>, compare_outcome_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  AgentGoalAccuracyWithReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'agent_goal_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, workflow_prompt: 'PydanticPrompt' = <factory>, compare_outcome_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      AgentGoalAccuracyWithReference
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'agent_goal_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, workflow_prompt: 'PydanticPrompt' = <factory>, compare_outcome_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  name = 'agent_goal_accuracy'
     |
     |  output_type = <MetricOutputType.BINARY: 'binary'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class AgentGoalAccuracyWithoutReference(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MultiTurnMetric)
     |  AgentGoalAccuracyWithoutReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'agent_goal_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, workflow_prompt: 'PydanticPrompt' = <factory>, compare_outcome_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  AgentGoalAccuracyWithoutReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'agent_goal_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, workflow_prompt: 'PydanticPrompt' = <factory>, compare_outcome_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      AgentGoalAccuracyWithoutReference
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'agent_goal_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, workflow_prompt: 'PydanticPrompt' = <factory>, compare_outcome_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  name = 'agent_goal_accuracy'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class AnswerAccuracy(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  AnswerAccuracy(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'nv_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |
     |  Measures answer accuracy compared to ground truth given a user_input.
     |  This metric averages two distinct judge prompts to evaluate.
     |
     |  Top10, Zero-shoot LLM-as-a-Judge Leaderboard:
     |  1)- nvidia/Llama-3_3-Nemotron-Super-49B-v1
     |  2)- mistralai/mixtral-8x22b-instruct-v0.1
     |  3)- mistralai/mixtral-8x7b-instruct-v0.1
     |  4)- meta/llama-3.1-70b-instruct
     |  5)- meta/llama-3.3-70b-instruct
     |  6)- meta/llama-3.1-405b-instruct
     |  7)- mistralai/mistral-nemo-12b-instruct
     |  8)- nvidia/llama-3.1-nemotron-70b-instruct
     |  9)- meta/llama-3.1-8b-instruct
     |  10)- google/gemma-2-2b-it
     |  The top1 LB model have high correlation with human judges (~0.92).
     |
     |  Attributes
     |  ----------
     |  name: string
     |      The name of the metrics
     |
     |  answer_accuracy:
     |      The AnswerAccuracy object
     |
     |  Method resolution order:
     |      AnswerAccuracy
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'nv_accuracy', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  average_scores(self, score0, score1)
     |
     |  process_score(self, response)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  name = 'nv_accuracy'
     |
     |  retry = 5
     |
     |  template_accuracy1 = 'Instruction: You are a world class state of the ...
     |
     |  template_accuracy2 = 'I will rate the User Answer in comparison to the...
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class AnswerCorrectness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MetricWithEmbeddings, ragas.metrics.base.SingleTurnMetric)
     |  AnswerCorrectness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_correctness', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, correctness_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, weights: 'list[float]' = <factory>, beta: 'float' = 1.0, answer_similarity: 't.Optional[AnswerSimilarity]' = None, max_retries: 'int' = 1) -> None
     |
     |  Measures answer correctness compared to ground truth as a combination of
     |  factuality and semantic similarity.
     |
     |  Attributes
     |  ----------
     |  name: string
     |      The name of the metrics
     |  weights:
     |      a list of two weights corresponding to factuality and semantic similarity
     |      Defaults [0.75, 0.25]
     |  answer_similarity:
     |      The AnswerSimilarity object
     |
     |  Method resolution order:
     |      AnswerCorrectness
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.MetricWithEmbeddings
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_correctness', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, correctness_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, weights: 'list[float]' = <factory>, beta: 'float' = 1.0, answer_similarity: 't.Optional[AnswerSimilarity]' = None, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig')
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'embeddings', 'llm', 'o...
     |
     |  answer_similarity = None
     |
     |  beta = 1.0
     |
     |  max_retries = 1
     |
     |  name = 'answer_correctness'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  embeddings = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class AnswerRelevancy(ResponseRelevancy)
     |  AnswerRelevancy(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_relevancy', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, question_generation: 'PydanticPrompt' = ResponseRelevancePrompt(instruction=Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, "I don't know" or "I'm not sure" are noncommittal answers, examples=[(ResponseRelevanceInput(response='Albert Einstein was born in Germany.'), ResponseRelevanceOutput(question='Where was Albert Einstein born?', noncommittal=0)), (ResponseRelevanceInput(response="I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. "), ResponseRelevanceOutput(question='What was the groundbreaking feature of the smartphone invented in 2023?', noncommittal=1))], language=english), strictness: 'int' = 3) -> None
     |
     |  Method resolution order:
     |      AnswerRelevancy
     |      ResponseRelevancy
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.MetricWithEmbeddings
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ResponseRelevancy:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_relevancy', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, question_generation: 'PydanticPrompt' = ResponseRelevancePrompt(instruction=Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, "I don't know" or "I'm not sure" are noncommittal answers, examples=[(ResponseRelevanceInput(response='Albert Einstein was born in Germany.'), ResponseRelevanceOutput(question='Where was Albert Einstein born?', noncommittal=0)), (ResponseRelevanceInput(response="I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. "), ResponseRelevanceOutput(question='What was the groundbreaking feature of the smartphone invented in 2023?', noncommittal=1))], language=english), strictness: 'int' = 3) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  calculate_similarity(self, question: 'str', generated_questions: 'list[str]')
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ResponseRelevancy:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'embeddings', 'llm', 'o...
     |
     |  name = 'answer_relevancy'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  question_generation = ResponseRelevancePrompt(instruction=Generate a q...
     |
     |  strictness = 3
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  embeddings = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class AnswerSimilarity(SemanticSimilarity)
     |  AnswerSimilarity(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_similarity', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, is_cross_encoder: 'bool' = False, threshold: 't.Optional[float]' = None) -> None
     |
     |  AnswerSimilarity(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_similarity', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, is_cross_encoder: 'bool' = False, threshold: 't.Optional[float]' = None)
     |
     |  Method resolution order:
     |      AnswerSimilarity
     |      SemanticSimilarity
     |      ragas.metrics.base.MetricWithEmbeddings
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_similarity', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, is_cross_encoder: 'bool' = False, threshold: 't.Optional[float]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'name': 'str'}
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'embeddings', 'is_cross...
     |
     |  name = 'answer_similarity'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from SemanticSimilarity:
     |
     |  __post_init__(self)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from SemanticSimilarity:
     |
     |  is_cross_encoder = False
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  threshold = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  init(self, run_config: 'RunConfig')
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  embeddings = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class AspectCritic(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
     |  AspectCritic(name: 'str', definition: 'str', llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, strictness: 'int' = 1, max_retries: 'int' = 1)
     |
     |  Judges the submission to give binary results using the criteria specified
     |  in the metric definition.
     |
     |  Attributes
     |  ----------
     |  name: str
     |      name of the metrics
     |  definition: str
     |      criteria to judge the submission, example "Is the submission spreading
     |      fake information?"
     |  strictness: int
     |      The number of times self consistency checks is made. Final judgement is
     |      made using majority vote.
     |
     |  Method resolution order:
     |      AspectCritic
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, name: 'str', definition: 'str', llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, strictness: 'int' = 1, max_retries: 'int' = 1)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self) -> 'str'
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  definition
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.Metric:
     |
     |  name = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    BaseMetric = class SimpleBaseMetric(abc.ABC)
     |  BaseMetric(name: 'str', allowed_values: 'AllowedValuesType' = <factory>) -> None
     |
     |  Base class for simple metrics that return MetricResult objects.
     |
     |  This class provides the foundation for metrics that evaluate inputs
     |  and return structured MetricResult objects containing scores and reasoning.
     |
     |  Attributes
     |  ----------
     |  name : str
     |      The name of the metric.
     |  allowed_values : AllowedValuesType
     |      Allowed values for the metric output. Can be a list of strings for
     |      discrete metrics, a tuple of floats for numeric metrics, or an integer
     |      for ranking metrics.
     |
     |  Examples
     |  --------
     |  >>> from ragas.metrics import discrete_metric
     |  >>>
     |  >>> @discrete_metric(name="sentiment", allowed_values=["positive", "negative"])
     |  >>> def sentiment_metric(user_input: str, response: str) -> str:
     |  ...     return "positive" if "good" in response else "negative"
     |  >>>
     |  >>> result = sentiment_metric(user_input="How are you?", response="I'm good!")
     |  >>> print(result.value)  # "positive"
     |
     |  Method resolution order:
     |      SimpleBaseMetric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, name: 'str', allowed_values: 'AllowedValuesType' = <factory>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  async abatch_score(self, inputs: 't.List[t.Dict[str, t.Any]]') -> "t.List['MetricResult']"
     |      Asynchronously calculate scores for a batch of inputs in parallel.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  async ascore(self, **kwargs) -> "'MetricResult'"
     |      Asynchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  batch_score(self, inputs: 't.List[t.Dict[str, t.Any]]') -> "t.List['MetricResult']"
     |      Synchronously calculate scores for a batch of inputs.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  score(self, **kwargs) -> "'MetricResult'"
     |      Synchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset({'ascore', 'score'})
     |
     |  __annotations__ = {'allowed_values': 'AllowedValuesType', 'name': 'str...
     |
     |  __dataclass_fields__ = {'allowed_values': Field(name='allowed_values',...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('name', 'allowed_values')

    class BleuScore(ragas.metrics.base.SingleTurnMetric)
     |  BleuScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'bleu_score', kwargs: Dict[str, Any] = <factory>) -> None
     |
     |  BleuScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'bleu_score', kwargs: Dict[str, Any] = <factory>)
     |
     |  Method resolution order:
     |      BleuScore
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'bleu_score', kwargs: Dict[str, Any] = <factory>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'kwargs')
     |
     |  name = 'bleu_score'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class ChrfScore(ragas.metrics.base.SingleTurnMetric)
     |  ChrfScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'chrf_score', kwargs: Dict[str, Any] = <factory>) -> None
     |
     |  ChrfScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'chrf_score', kwargs: Dict[str, Any] = <factory>)
     |
     |  Method resolution order:
     |      ChrfScore
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'chrf_score', kwargs: Dict[str, Any] = <factory>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'kwargs')
     |
     |  name = 'chrf_score'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class ContextEntityRecall(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  ContextEntityRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_entity_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_entity_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  Calculates recall based on entities present in ground truth and context.
     |  Let CN be the set of entities present in context,
     |  GN be the set of entities present in the ground truth.
     |
     |  Then we define can the context entity recall as follows:
     |  Context Entity recall = | CN \u2229 GN | / | GN |
     |
     |  If this quantity is 1, we can say that the retrieval mechanism has
     |  retrieved context which covers all entities present in the ground truth,
     |  thus being a useful retrieval. Thus this can be used to evaluate retrieval
     |  mechanisms in specific use cases where entities matter, for example, a
     |  tourism help chatbot.
     |
     |  Attributes
     |  ----------
     |  name : str
     |  batch_size : int
     |      Batch size for openai completion.
     |
     |  Method resolution order:
     |      ContextEntityRecall
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_entity_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_entity_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  async get_entities(self, text: 'str', callbacks: 'Callbacks') -> 'EntitiesList'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  name = 'context_entity_recall'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class ContextPrecision(LLMContextPrecisionWithReference)
     |  ContextPrecision(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_precision', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  ContextPrecision(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_precision', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      ContextPrecision
     |      LLMContextPrecisionWithReference
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_precision', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'name': 'str'}
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  name = 'context_precision'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from LLMContextPrecisionWithReference:
     |
     |  max_retries = 1
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class ContextRecall(LLMContextRecall)
     |  ContextRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, context_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  ContextRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, context_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      ContextRecall
     |      LLMContextRecall
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, context_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'name': 'str'}
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  name = 'context_recall'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from LLMContextRecall:
     |
     |  max_retries = 1
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class ContextRelevance(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  ContextRelevance(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'nv_context_relevance', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |
     |  Parameters:
     |  Score the relevance of the retrieved contexts be based on the user input.
     |
     |  Input:
     |      data: list of Dicts with keys: user_input, retrieved_contexts
     |  Output:
     |      0.0: retrieved_contexts is not relevant for the user_input
     |      0.5: retrieved_contexts is partially relevant for the user_input
     |      1.0: retrieved_contexts is fully relevant for the user_input
     |
     |  Method resolution order:
     |      ContextRelevance
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'nv_context_relevance', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  average_scores(self, score0, score1)
     |
     |  process_score(self, response)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  name = 'nv_context_relevance'
     |
     |  retry = 5
     |
     |  template_relevance1 = '### Instructions\n\nYou are a world class exper...
     |
     |  template_relevance2 = 'As a specially designed expert to assess the re...
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class ContextUtilization(LLMContextPrecisionWithoutReference)
     |  ContextUtilization(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_utilization', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  ContextUtilization(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_utilization', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      ContextUtilization
     |      LLMContextPrecisionWithoutReference
     |      LLMContextPrecisionWithReference
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_utilization', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'name': 'str'}
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  name = 'context_utilization'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from LLMContextPrecisionWithReference:
     |
     |  max_retries = 1
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class DataCompyScore(ragas.metrics.base.SingleTurnMetric)
     |  DataCompyScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'data_compare_score', mode: Literal['rows', 'columns'] = 'rows', metric: Literal['precision', 'recall', 'f1'] = 'f1') -> None
     |
     |  DataCompyScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'data_compare_score', mode: Literal['rows', 'columns'] = 'rows', metric: Literal['precision', 'recall', 'f1'] = 'f1')
     |
     |  Method resolution order:
     |      DataCompyScore
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'data_compare_score', mode: Literal['rows', 'columns'] = 'rows', metric: Literal['precision', 'recall', 'f1'] = 'f1') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'mode', 'metric')
     |
     |  metric = 'f1'
     |
     |  mode = 'rows'
     |
     |  name = 'data_compare_score'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class DiscreteMetric(ragas.metrics.base.SimpleLLMMetric, ragas.metrics.validators.DiscreteValidator)
     |  DiscreteMetric(name: 'str', allowed_values: List[str] = <factory>, prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |
     |  Metric for categorical/discrete evaluations with predefined allowed values.
     |
     |  This class is used for metrics that output categorical values like
     |  "pass/fail", "good/bad/excellent", or custom discrete categories.
     |  Uses the instructor library for structured LLM outputs.
     |
     |  Attributes
     |  ----------
     |  allowed_values : List[str]
     |      List of allowed categorical values the metric can output.
     |      Default is ["pass", "fail"].
     |  llm : Optional[BaseRagasLLM]
     |      The language model instance for evaluation. Can be created using llm_factory().
     |  prompt : Optional[Union[str, Prompt]]
     |      The prompt template for the metric. Should contain placeholders for
     |      evaluation inputs that will be formatted at runtime.
     |
     |  Examples
     |  --------
     |  >>> from ragas.metrics import DiscreteMetric
     |  >>> from ragas.llms import llm_factory
     |  >>> from openai import OpenAI
     |  >>>
     |  >>> # Create an LLM instance
     |  >>> client = OpenAI(api_key="your-api-key")
     |  >>> llm = llm_factory("gpt-4o-mini", client=client)
     |  >>>
     |  >>> # Create a custom discrete metric
     |  >>> metric = DiscreteMetric(
     |  ...     name="quality_check",
     |  ...     llm=llm,
     |  ...     prompt="Check the quality of the response: {response}. Return 'excellent', 'good', or 'poor'.",
     |  ...     allowed_values=["excellent", "good", "poor"]
     |  ... )
     |  >>>
     |  >>> # Score with the metric
     |  >>> result = metric.score(
     |  ...     llm=llm,
     |  ...     response="This is a great response!"
     |  ... )
     |  >>> print(result.value)  # Output: "excellent" or similar
     |
     |  Method resolution order:
     |      DiscreteMetric
     |      ragas.metrics.base.SimpleLLMMetric
     |      ragas.metrics.base.SimpleBaseMetric
     |      ragas.metrics.validators.DiscreteValidator
     |      ragas.metrics.validators.BaseValidator
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, name: 'str', allowed_values: List[str] = <factory>, prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  get_correlation(self, gold_labels: List[str], predictions: List[str]) -> float
     |      Calculate the correlation between gold labels and predictions.
     |      This is a placeholder method and should be implemented based on the specific metric.
     |
     |  ----------------------------------------------------------------------
     |  Class methods defined here:
     |
     |  load(path: str, embedding_model: Optional[ForwardRef('EmbeddingModelType')] = None) -> 'DiscreteMetric'
     |      Load a DiscreteMetric from a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str
     |          File path to load from. Supports .gz compressed files.
     |      embedding_model : Optional[Any]
     |          Embedding model for DynamicFewShotPrompt. Required if the original used one.
     |
     |      Returns:
     |      --------
     |      DiscreteMetric
     |          Loaded metric instance
     |
     |      Raises:
     |      -------
     |      ValueError
     |          If file cannot be loaded or is not a DiscreteMetric
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'allowed_values': typing.List[str]}
     |
     |  __dataclass_fields__ = {'_response_model': Field(name='_response_model...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=False,eq=True,o...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('name', 'allowed_values', 'prompt')
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SimpleLLMMetric:
     |
     |  __repr__(self) -> 'str'
     |      Return a clean string representation of the metric.
     |
     |  async abatch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Asynchronously calculate scores for a batch of inputs in parallel.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  align(self, train_dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          train_dataset: train_dataset to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |
     |      Align the metric with the specified experiments by different optimization methods.
     |
     |  align_and_validate(self, dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", llm: "'BaseRagasLLM'", test_size: 'float' = 0.2, random_state: 'int' = 42, **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          dataset: experiment to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |          llm: The LLM instance to use for scoring.
     |
     |      Align the metric with the specified experiments and validate it against a gold standard experiment.
     |      This method combines alignment and validation into a single step.
     |
     |  async ascore(self, **kwargs) -> "'MetricResult'"
     |      Asynchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  batch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Synchronously calculate scores for a batch of inputs.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  get_variables(self) -> 't.List[str]'
     |
     |  save(self, path: 't.Optional[str]' = None) -> 'None'
     |      Save the metric configuration to a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str, optional
     |          File path to save to. If not provided, saves to "./{metric.name}.json"
     |          Use .gz extension for compression.
     |
     |      Note:
     |      -----
     |      If the metric has a response_model, its schema will be saved for reference
     |      but the model itself cannot be serialized. You'll need to provide it when loading.
     |
     |      Examples:
     |      ---------
     |      All these work:
     |      >>> metric.save()                      # \u2192 ./response_quality.json
     |      >>> metric.save("custom.json")         # \u2192 ./custom.json
     |      >>> metric.save("/path/to/metrics/")   # \u2192 /path/to/metrics/response_quality.json
     |      >>> metric.save("no_extension")        # \u2192 ./no_extension.json
     |      >>> metric.save("compressed.json.gz")  # \u2192 ./compressed.json.gz (compressed)
     |
     |  score(self, **kwargs) -> "'MetricResult'"
     |      Synchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  validate_alignment(self, llm: "'BaseRagasLLM'", test_dataset: "'Dataset'", mapping: 't.Dict[str, str]' = {})
     |      Args:
     |          llm: The LLM instance to use for scoring.
     |          test_dataset: An Dataset instance containing the gold standard scores.
     |          mapping: A dictionary mapping variable names expected by metrics to their corresponding names in the gold experiment.
     |
     |      Validate the alignment of the metric by comparing the scores against a gold standard experiment.
     |      This method computes the Cohen's Kappa score and agreement rate between the gold standard scores and
     |      the predicted scores from the metric.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.SimpleLLMMetric:
     |
     |  prompt = None
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.SimpleBaseMetric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.validators.DiscreteValidator:
     |
     |  validate_result_value(self, result_value: Any) -> Optional[str]
     |      Validate that result value is in the allowed discrete values.

    class DistanceMeasure(enum.Enum)
     |  DistanceMeasure(*values)
     |
     |  Method resolution order:
     |      DistanceMeasure
     |      enum.Enum
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  HAMMING = <DistanceMeasure.HAMMING: 'hamming'>
     |
     |  JARO = <DistanceMeasure.JARO: 'jaro'>
     |
     |  JARO_WINKLER = <DistanceMeasure.JARO_WINKLER: 'jaro_winkler'>
     |
     |  LEVENSHTEIN = <DistanceMeasure.LEVENSHTEIN: 'levenshtein'>
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from enum.Enum:
     |
     |  name
     |      The name of the Enum member.
     |
     |  value
     |      The value of the Enum member.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from enum.EnumType:
     |
     |  __contains__(value)
     |      Return True if `value` is in `cls`.
     |
     |      `value` is in `cls` if:
     |      1) `value` is a member of `cls`, or
     |      2) `value` is the value of one of the `cls`'s members.
     |      3) `value` is a pseudo-member (flags)
     |
     |  __getitem__(name)
     |      Return the member matching `name`.
     |
     |  __iter__()
     |      Return members in definition order.
     |
     |  __len__()
     |      Return the number of members (no aliases)
     |
     |  ----------------------------------------------------------------------
     |  Readonly properties inherited from enum.EnumType:
     |
     |  __members__
     |      Returns a mapping of member name->value.
     |
     |      This mapping lists all enum members, including aliases. Note that this
     |      is a read-only view of the internal mapping.

    class ExactMatch(ragas.metrics.base.SingleTurnMetric)
     |  ExactMatch(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'exact_match') -> None
     |
     |  ExactMatch(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'exact_match')
     |
     |  Method resolution order:
     |      ExactMatch
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'exact_match') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name')
     |
     |  name = 'exact_match'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class FactualCorrectness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  FactualCorrectness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'factual_correctness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['precision', 'recall', 'f1']" = 'f1', beta: 'float' = 1.0, atomicity: "t.Literal['low', 'high']" = 'low', coverage: "t.Literal['low', 'high']" = 'low', claim_decomposition_prompt: 'PydanticPrompt' = <factory>, nli_prompt: 'PydanticPrompt' = <factory>, language: 'str' = 'english') -> None
     |
     |  FactualCorrectness is a metric class that evaluates the factual correctness of responses
     |  generated by a language model. It uses claim decomposition and natural language inference (NLI)
     |  to verify the claims made in the responses against reference texts.
     |
     |  Attributes:
     |      name (str): The name of the metric, default is "factual_correctness".
     |      _required_columns (Dict[MetricType, Set[str]]): A dictionary specifying the required columns
     |          for each metric type. Default is {"SINGLE_TURN": {"response", "reference"}}.
     |      mode (Literal["precision", "recall", "f1"]): The mode of evaluation, can be "precision",
     |          "recall", or "f1". Default is "f1".
     |      beta (float): The beta value used for the F1 score calculation. A beta > 1 gives more weight
     |          to recall, while beta < 1 favors precision. Default is 1.0.
     |      atomicity (Literal["low", "high"]): The level of atomicity for claim decomposition. Default is "low".
     |      coverage (Literal["low", "high"]): The level of coverage for claim decomposition. Default is "low".
     |      claim_decomposition_prompt (PydanticPrompt): The prompt used for claim decomposition.
     |      nli_prompt (PydanticPrompt): The prompt used for natural language inference (NLI).
     |
     |  Method resolution order:
     |      FactualCorrectness
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'factual_correctness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['precision', 'recall', 'f1']" = 'f1', beta: 'float' = 1.0, atomicity: "t.Literal['low', 'high']" = 'low', coverage: "t.Literal['low', 'high']" = 'low', claim_decomposition_prompt: 'PydanticPrompt' = <factory>, nli_prompt: 'PydanticPrompt' = <factory>, language: 'str' = 'english') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  async decompose_and_verify_claims(self, reference: 'str', response: 'str', callbacks: 'Callbacks') -> 'np.ndarray'
     |
     |  async decompose_claims(self, response: 'str', callbacks: 'Callbacks') -> 't.List[str]'
     |
     |  async verify_claims(self, premise: 'str', hypothesis_list: 't.List[str]', callbacks: 'Callbacks') -> 'np.ndarray'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  atomicity = 'low'
     |
     |  beta = 1.0
     |
     |  coverage = 'low'
     |
     |  language = 'english'
     |
     |  mode = 'f1'
     |
     |  name = 'factual_correctness'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class Faithfulness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  Faithfulness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithfulness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  Faithfulness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithfulness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      Faithfulness
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithfulness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  name = 'faithfulness'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class FaithfulnesswithHHEM(Faithfulness)
     |  FaithfulnesswithHHEM(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithfulness_with_hhem', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1, device: 'str' = 'cpu', batch_size: 'int' = 10) -> None
     |
     |  FaithfulnesswithHHEM(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithfulness_with_hhem', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1, device: 'str' = 'cpu', batch_size: 'int' = 10)
     |
     |  Method resolution order:
     |      FaithfulnesswithHHEM
     |      Faithfulness
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithfulness_with_hhem', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1, device: 'str' = 'cpu', batch_size: 'int' = 10) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'batch_size': 'int', 'device': 'str', 'name': 'str'...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  batch_size = 10
     |
     |  device = 'cpu'
     |
     |  name = 'faithfulness_with_hhem'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Faithfulness:
     |
     |  max_retries = 1
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class IDBasedContextPrecision(ragas.metrics.base.SingleTurnMetric)
     |  IDBasedContextPrecision(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'id_based_context_precision', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>) -> None
     |
     |  Calculates context precision by directly comparing retrieved context IDs with reference context IDs.
     |  The score represents what proportion of the retrieved context IDs are actually relevant (present in reference).
     |
     |  This metric works with both string and integer IDs.
     |
     |  Attributes
     |  ----------
     |  name : str
     |      Name of the metric
     |
     |  Method resolution order:
     |      IDBasedContextPrecision
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'id_based_context_precision', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'output_type')
     |
     |  name = 'id_based_context_precision'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class IDBasedContextRecall(ragas.metrics.base.SingleTurnMetric)
     |  IDBasedContextRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'id_based_context_recall', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>) -> None
     |
     |  Calculates context recall by directly comparing retrieved context IDs with reference context IDs.
     |  The score represents what proportion of the reference IDs were successfully retrieved.
     |
     |  This metric works with both string and integer IDs.
     |
     |  Attributes
     |  ----------
     |  name : str
     |      Name of the metric
     |
     |  Method resolution order:
     |      IDBasedContextRecall
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'id_based_context_recall', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'output_type')
     |
     |  name = 'id_based_context_recall'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class InstanceRubrics(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
     |  InstanceRubrics(name: 'str' = 'instance_rubrics', llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.DISCRETE: 'discrete'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      InstanceRubrics
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, name: 'str' = 'instance_rubrics', llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.DISCRETE: 'discrete'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, max_retries: 'int' = 1)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self) -> 'str'
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.Metric:
     |
     |  name = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class LLMContextPrecisionWithReference(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  LLMContextPrecisionWithReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_context_precision_with_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  Average Precision is a metric that evaluates whether all of the
     |  relevant items selected by the model are ranked higher or not.
     |
     |  Attributes
     |  ----------
     |  name : str
     |  evaluation_mode: EvaluationMode
     |  context_precision_prompt: Prompt
     |
     |  Method resolution order:
     |      LLMContextPrecisionWithReference
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_context_precision_with_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  name = 'llm_context_precision_with_reference'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class LLMContextPrecisionWithoutReference(LLMContextPrecisionWithReference)
     |  LLMContextPrecisionWithoutReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_context_precision_without_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  LLMContextPrecisionWithoutReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_context_precision_without_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      LLMContextPrecisionWithoutReference
     |      LLMContextPrecisionWithReference
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_context_precision_without_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, context_precision_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  name = 'llm_context_precision_without_reference'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from LLMContextPrecisionWithReference:
     |
     |  max_retries = 1
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class LLMContextRecall(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  LLMContextRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, context_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  Estimates context recall by estimating TP and FN using annotated answer and
     |  retrieved context.
     |
     |  Attributes
     |  ----------
     |  name : str
     |
     |  Method resolution order:
     |      LLMContextRecall
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'context_recall', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, context_recall_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  name = 'context_recall'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    LLMMetric = class SimpleLLMMetric(SimpleBaseMetric)
     |  LLMMetric(name: 'str', allowed_values: 'AllowedValuesType' = <factory>, prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |
     |  LLM-based metric that uses prompts to generate structured responses.
     |
     |  Method resolution order:
     |      SimpleLLMMetric
     |      SimpleBaseMetric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, name: 'str', allowed_values: 'AllowedValuesType' = <factory>, prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self) -> 'str'
     |      Return a clean string representation of the metric.
     |
     |  async abatch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Asynchronously calculate scores for a batch of inputs in parallel.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  align(self, train_dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          train_dataset: train_dataset to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |
     |      Align the metric with the specified experiments by different optimization methods.
     |
     |  align_and_validate(self, dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", llm: "'BaseRagasLLM'", test_size: 'float' = 0.2, random_state: 'int' = 42, **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          dataset: experiment to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |          llm: The LLM instance to use for scoring.
     |
     |      Align the metric with the specified experiments and validate it against a gold standard experiment.
     |      This method combines alignment and validation into a single step.
     |
     |  async ascore(self, **kwargs) -> "'MetricResult'"
     |      Asynchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  batch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Synchronously calculate scores for a batch of inputs.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  get_correlation(self, gold_labels: 't.List[str]', predictions: 't.List[str]') -> 'float'
     |      Calculate the correlation between gold scores and predicted scores.
     |      This is a placeholder method and should be implemented based on the specific metric.
     |
     |  get_variables(self) -> 't.List[str]'
     |
     |  save(self, path: 't.Optional[str]' = None) -> 'None'
     |      Save the metric configuration to a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str, optional
     |          File path to save to. If not provided, saves to "./{metric.name}.json"
     |          Use .gz extension for compression.
     |
     |      Note:
     |      -----
     |      If the metric has a response_model, its schema will be saved for reference
     |      but the model itself cannot be serialized. You'll need to provide it when loading.
     |
     |      Examples:
     |      ---------
     |      All these work:
     |      >>> metric.save()                      # \u2192 ./response_quality.json
     |      >>> metric.save("custom.json")         # \u2192 ./custom.json
     |      >>> metric.save("/path/to/metrics/")   # \u2192 /path/to/metrics/response_quality.json
     |      >>> metric.save("no_extension")        # \u2192 ./no_extension.json
     |      >>> metric.save("compressed.json.gz")  # \u2192 ./compressed.json.gz (compressed)
     |
     |  score(self, **kwargs) -> "'MetricResult'"
     |      Synchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  validate_alignment(self, llm: "'BaseRagasLLM'", test_dataset: "'Dataset'", mapping: 't.Dict[str, str]' = {})
     |      Args:
     |          llm: The LLM instance to use for scoring.
     |          test_dataset: An Dataset instance containing the gold standard scores.
     |          mapping: A dictionary mapping variable names expected by metrics to their corresponding names in the gold experiment.
     |
     |      Validate the alignment of the metric by comparing the scores against a gold standard experiment.
     |      This method computes the Cohen's Kappa score and agreement rate between the gold standard scores and
     |      the predicted scores from the metric.
     |
     |  ----------------------------------------------------------------------
     |  Class methods defined here:
     |
     |  load(path: 'str', response_model: "t.Optional[t.Type['BaseModel']]" = None, embedding_model: "t.Optional['EmbeddingModelType']" = None) -> "'SimpleLLMMetric'"
     |      Load a metric from a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str
     |          File path to load from. Supports .gz compressed files.
     |      response_model : Optional[Type[BaseModel]]
     |          Pydantic model to use for response validation. Required for custom SimpleLLMMetrics.
     |      embedding_model : Optional[Any]
     |          Embedding model for DynamicFewShotPrompt. Required if the original used one.
     |
     |      Returns:
     |      --------
     |      SimpleLLMMetric
     |          Loaded metric instance
     |
     |      Raises:
     |      -------
     |      ValueError
     |          If file cannot be loaded, is invalid, or missing required models
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset({'get_correlation'})
     |
     |  __annotations__ = {'_response_model': "t.Type['BaseModel']", 'prompt':...
     |
     |  __dataclass_fields__ = {'_response_model': Field(name='_response_model...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=False,eq=True,o...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('name', 'allowed_values', 'prompt')
     |
     |  prompt = None
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from SimpleBaseMetric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object

    class LLMSQLEquivalence(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  LLMSQLEquivalence(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_sql_equivalence_with_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, equivalence_prompt: 'PydanticPrompt' = EquivalencePrompt(instruction=
     |      Explain and compare two SQL queries (Q1 and Q2) based on the provided database schema. First, explain each query, then determine if they have significant logical differences.
     |      , examples=[(EquivalenceInput(reference='SELECT id, name FROM users WHERE active = 1;', response='SELECT id, name FROM users WHERE active = true;', database_schema='\n                    Table users:\n                    - id: INT\n                    - name: VARCHAR\n                    - active: BOOLEAN\n                '), EquivalenceOutput(response_query_explaination='The generated SQL query retrieves the id and name of users where the active field is true.', reference_query_explaination='The reference SQL query retrieves the id and name of users where the active field equals 1.', equivalence=True))], language=english)) -> None
     |
     |  LLMSQLEquivalence(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_sql_equivalence_with_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, equivalence_prompt: 'PydanticPrompt' = EquivalencePrompt(instruction=
     |  Explain and compare two SQL queries (Q1 and Q2) based on the provided database schema. First, explain each query, then determine if they have significant logical differences.
     |  , examples=[(EquivalenceInput(reference='SELECT id, name FROM users WHERE active = 1;', response='SELECT id, name FROM users WHERE active = true;', database_schema='\n                    Table users:\n                    - id: INT\n                    - name: VARCHAR\n                    - active: BOOLEAN\n                '), EquivalenceOutput(response_query_explaination='The generated SQL query retrieves the id and name of users where the active field is true.', reference_query_explaination='The reference SQL query retrieves the id and name of users where the active field equals 1.', equivalence=True))], language=english))
     |
     |  Method resolution order:
     |      LLMSQLEquivalence
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'llm_sql_equivalence_with_reference', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.BINARY: 'binary'>, equivalence_prompt: 'PydanticPrompt' = EquivalencePrompt(instruction=
     |      Explain and compare two SQL queries (Q1 and Q2) based on the provided database schema. First, explain each query, then determine if they have significant logical differences.
     |      , examples=[(EquivalenceInput(reference='SELECT id, name FROM users WHERE active = 1;', response='SELECT id, name FROM users WHERE active = true;', database_schema='\n                    Table users:\n                    - id: INT\n                    - name: VARCHAR\n                    - active: BOOLEAN\n                '), EquivalenceOutput(response_query_explaination='The generated SQL query retrieves the id and name of users where the active field is true.', reference_query_explaination='The reference SQL query retrieves the id and name of users where the active field equals 1.', equivalence=True))], language=english)) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  equivalence_prompt = EquivalencePrompt(instruction=
     |      Explain and c....
     |
     |  name = 'llm_sql_equivalence_with_reference'
     |
     |  output_type = <MetricOutputType.BINARY: 'binary'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class Metric(abc.ABC)
     |  Metric(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '') -> None
     |
     |  Abstract base class for metrics in Ragas.
     |
     |  Attributes
     |  ----------
     |  name : str
     |      The name of the metric.
     |  required_columns : Dict[str, Set[str]]
     |      A dictionary mapping metric type names to sets of required column names. This is
     |      a property and raises `ValueError` if columns are not in `VALID_COLUMNS`.
     |
     |  Method resolution order:
     |      Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset({'init'})
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name')
     |
     |  name = ''

    class MetricOutputType(enum.Enum)
     |  MetricOutputType(*values)
     |
     |  Method resolution order:
     |      MetricOutputType
     |      enum.Enum
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  BINARY = <MetricOutputType.BINARY: 'binary'>
     |
     |  CONTINUOUS = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  DISCRETE = <MetricOutputType.DISCRETE: 'discrete'>
     |
     |  RANKING = <MetricOutputType.RANKING: 'ranking'>
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from enum.Enum:
     |
     |  name
     |      The name of the Enum member.
     |
     |  value
     |      The value of the Enum member.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from enum.EnumType:
     |
     |  __contains__(value)
     |      Return True if `value` is in `cls`.
     |
     |      `value` is in `cls` if:
     |      1) `value` is a member of `cls`, or
     |      2) `value` is the value of one of the `cls`'s members.
     |      3) `value` is a pseudo-member (flags)
     |
     |  __getitem__(name)
     |      Return the member matching `name`.
     |
     |  __iter__()
     |      Return members in definition order.
     |
     |  __len__()
     |      Return the number of members (no aliases)
     |
     |  ----------------------------------------------------------------------
     |  Readonly properties inherited from enum.EnumType:
     |
     |  __members__
     |      Returns a mapping of member name->value.
     |
     |      This mapping lists all enum members, including aliases. Note that this
     |      is a read-only view of the internal mapping.

    class MetricResult(builtins.object)
     |  MetricResult(value: Any, reason: Optional[str] = None, traces: Optional[Dict[str, Any]] = None)
     |
     |  Class to hold the result of a metric evaluation.
     |
     |  This class behaves like its underlying result value but still provides access
     |  to additional metadata like reasoning.
     |
     |  Works with:
     |  - DiscreteMetrics (string results)
     |  - NumericMetrics (float/int results)
     |  - RankingMetrics (list results)
     |
     |  Methods defined here:
     |
     |  __add__(self, other)
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __float__(self)
     |      # Numeric operations for numeric results (NumericMetric)
     |
     |  __ge__(self, other)
     |      Return self>=value.
     |
     |  __getattr__(self, name)
     |      Forward attribute access to the result object if it has that attribute.
     |
     |      This allows calling string methods on discrete results,
     |      numeric methods on numeric results, and list methods on ranking results.
     |
     |  __getitem__(self, key)
     |      # Container-like behaviors for list results (RankingMetric)
     |
     |  __gt__(self, other)
     |      Return self>value.
     |
     |  __init__(self, value: Any, reason: Optional[str] = None, traces: Optional[Dict[str, Any]] = None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __int__(self)
     |
     |  __iter__(self)
     |
     |  __json__(self)
     |      Return data for JSON serialization.
     |
     |      This method is used by json.dumps and other JSON serializers
     |      to convert MetricResult to a JSON-compatible format.
     |
     |  __le__(self, other)
     |      Return self<=value.
     |
     |  __len__(self)
     |
     |  __lt__(self, other)
     |      Return self<value.
     |
     |  __mul__(self, other)
     |
     |  __radd__(self, other)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  __rmul__(self, other)
     |
     |  __rsub__(self, other)
     |
     |  __rtruediv__(self, other)
     |
     |  __str__ = __repr__(self)
     |
     |  __sub__(self, other)
     |
     |  __truediv__(self, other)
     |
     |  to_dict(self)
     |      Convert the result to a dictionary.
     |
     |  ----------------------------------------------------------------------
     |  Class methods defined here:
     |
     |  __get_pydantic_core_schema__(_source_type: Any, _handler: pydantic.annotated_handlers.GetCoreSchemaHandler) -> Union[pydantic_core.core_schema.InvalidSchema, pydantic_core.core_schema.AnySchema, pydantic_core.core_schema.NoneSchema, pydantic_core.core_schema.BoolSchema, pydantic_core.core_schema.IntSchema, pydantic_core.core_schema.FloatSchema, pydantic_core.core_schema.DecimalSchema, pydantic_core.core_schema.StringSchema, pydantic_core.core_schema.BytesSchema, pydantic_core.core_schema.DateSchema, pydantic_core.core_schema.TimeSchema, pydantic_core.core_schema.DatetimeSchema, pydantic_core.core_schema.TimedeltaSchema, pydantic_core.core_schema.LiteralSchema, pydantic_core.core_schema.MissingSentinelSchema, pydantic_core.core_schema.EnumSchema, pydantic_core.core_schema.IsInstanceSchema, pydantic_core.core_schema.IsSubclassSchema, pydantic_core.core_schema.CallableSchema, pydantic_core.core_schema.ListSchema, pydantic_core.core_schema.TupleSchema, pydantic_core.core_schema.SetSchema, pydantic_core.core_schema.FrozenSetSchema, pydantic_core.core_schema.GeneratorSchema, pydantic_core.core_schema.DictSchema, pydantic_core.core_schema.AfterValidatorFunctionSchema, pydantic_core.core_schema.BeforeValidatorFunctionSchema, pydantic_core.core_schema.WrapValidatorFunctionSchema, pydantic_core.core_schema.PlainValidatorFunctionSchema, pydantic_core.core_schema.WithDefaultSchema, pydantic_core.core_schema.NullableSchema, pydantic_core.core_schema.UnionSchema, pydantic_core.core_schema.TaggedUnionSchema, pydantic_core.core_schema.ChainSchema, pydantic_core.core_schema.LaxOrStrictSchema, pydantic_core.core_schema.JsonOrPythonSchema, pydantic_core.core_schema.TypedDictSchema, pydantic_core.core_schema.ModelFieldsSchema, pydantic_core.core_schema.ModelSchema, pydantic_core.core_schema.DataclassArgsSchema, pydantic_core.core_schema.DataclassSchema, pydantic_core.core_schema.ArgumentsSchema, pydantic_core.core_schema.ArgumentsV3Schema, pydantic_core.core_schema.CallSchema, pydantic_core.core_schema.CustomErrorSchema, pydantic_core.core_schema.JsonSchema, pydantic_core.core_schema.UrlSchema, pydantic_core.core_schema.MultiHostUrlSchema, pydantic_core.core_schema.DefinitionsSchema, pydantic_core.core_schema.DefinitionReferenceSchema, pydantic_core.core_schema.UuidSchema, pydantic_core.core_schema.ComplexSchema]
     |      Generate a Pydantic core schema for MetricResult.
     |
     |      This custom schema handles different serialization behaviors:
     |      - For model_dump(): Returns the original MetricResult instance
     |      - For model_dump_json(): Converts to a JSON-compatible dict using __json__
     |
     |  validate(value: Any, info: pydantic_core.core_schema.ValidationInfo)
     |      Provide compatibility with older Pydantic versions.
     |
     |  ----------------------------------------------------------------------
     |  Readonly properties defined here:
     |
     |  value
     |      Get the raw result value.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __hash__ = None

    class MetricType(enum.Enum)
     |  MetricType(*values)
     |
     |  Enumeration of metric types in Ragas.
     |
     |  Attributes
     |  ----------
     |  SINGLE_TURN : str
     |      Represents a single-turn metric type.
     |  MULTI_TURN : str
     |      Represents a multi-turn metric type.
     |
     |  Method resolution order:
     |      MetricType
     |      enum.Enum
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  MULTI_TURN = <MetricType.MULTI_TURN: 'multi_turn'>
     |
     |  SINGLE_TURN = <MetricType.SINGLE_TURN: 'single_turn'>
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from enum.Enum:
     |
     |  name
     |      The name of the Enum member.
     |
     |  value
     |      The value of the Enum member.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from enum.EnumType:
     |
     |  __contains__(value)
     |      Return True if `value` is in `cls`.
     |
     |      `value` is in `cls` if:
     |      1) `value` is a member of `cls`, or
     |      2) `value` is the value of one of the `cls`'s members.
     |      3) `value` is a pseudo-member (flags)
     |
     |  __getitem__(name)
     |      Return the member matching `name`.
     |
     |  __iter__()
     |      Return members in definition order.
     |
     |  __len__()
     |      Return the number of members (no aliases)
     |
     |  ----------------------------------------------------------------------
     |  Readonly properties inherited from enum.EnumType:
     |
     |  __members__
     |      Returns a mapping of member name->value.
     |
     |      This mapping lists all enum members, including aliases. Note that this
     |      is a read-only view of the internal mapping.

    class MetricWithEmbeddings(Metric)
     |  MetricWithEmbeddings(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None) -> None
     |
     |  MetricWithEmbeddings(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None)
     |
     |  Method resolution order:
     |      MetricWithEmbeddings
     |      Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig')
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'embeddings': 't.Optional[t.Union[BaseRagasEmbeddin...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'embeddings')
     |
     |  embeddings = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Metric:
     |
     |  name = ''

    class MetricWithLLM(Metric, ragas.prompt.mixin.PromptMixin)
     |  MetricWithLLM(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |
     |  A metric class that uses a language model for evaluation.
     |
     |  Attributes
     |  ----------
     |  llm : Optional[BaseRagasLLM]
     |      The language model used for the metric. Both BaseRagasLLM and InstructorBaseRagasLLM
     |      are accepted at runtime via duck typing (both have compatible methods).
     |
     |  Method resolution order:
     |      MetricWithLLM
     |      Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'llm': 't.Optional[BaseRagasLLM]', 'output_type': '...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Metric:
     |
     |  name = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class MultiModalFaithfulness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  MultiModalFaithfulness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithful_rate', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, faithfulness_prompt: 'ImageTextPrompt' = MultiModalFaithfulnessPrompt(instruction=Please tell if a given piece of information is supported by the visual as well as textual context information. You need to answer with either True or False. Answer True if any of the image(s) and textual context supports the information, examples=[(FaithfulnessInput(response='Apple pie is generally double-crusted.', retrieved_contexts=['An apple pie is a fruit pie in which the principal filling ingredient is apples.', "Apple pie is often served with whipped cream, ice cream ('apple pie  la mode'), custard or cheddar cheese.", 'It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).']), FaithfulnessOutput(faithful=True)), (FaithfulnessInput(response='Apple pies tastes bad.', retrieved_contexts=['An apple pie is a fruit pie in which the principal filling ingredient is apples.', "Apple pie is often served with whipped cream, ice cream ('apple pie  la mode'), custard or cheddar cheese.", 'It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).']), FaithfulnessOutput(faithful=False))], language=english)) -> None
     |
     |  MultiModalFaithfulness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithful_rate', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, faithfulness_prompt: 'ImageTextPrompt' = MultiModalFaithfulnessPrompt(instruction=Please tell if a given piece of information is supported by the visual as well as textual context information. You need to answer with either True or False. Answer True if any of the image(s) and textual context supports the information, examples=[(FaithfulnessInput(response='Apple pie is generally double-crusted.', retrieved_contexts=['An apple pie is a fruit pie in which the principal filling ingredient is apples.', "Apple pie is often served with whipped cream, ice cream ('apple pie  la mode'), custard or cheddar cheese.", 'It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).']), FaithfulnessOutput(faithful=True)), (FaithfulnessInput(response='Apple pies tastes bad.', retrieved_contexts=['An apple pie is a fruit pie in which the principal filling ingredient is apples.', "Apple pie is often served with whipped cream, ice cream ('apple pie  la mode'), custard or cheddar cheese.", 'It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).']), FaithfulnessOutput(faithful=False))], language=english))
     |
     |  Method resolution order:
     |      MultiModalFaithfulness
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'faithful_rate', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, faithfulness_prompt: 'ImageTextPrompt' = MultiModalFaithfulnessPrompt(instruction=Please tell if a given piece of information is supported by the visual as well as textual context information. You need to answer with either True or False. Answer True if any of the image(s) and textual context supports the information, examples=[(FaithfulnessInput(response='Apple pie is generally double-crusted.', retrieved_contexts=['An apple pie is a fruit pie in which the principal filling ingredient is apples.', "Apple pie is often served with whipped cream, ice cream ('apple pie  la mode'), custard or cheddar cheese.", 'It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).']), FaithfulnessOutput(faithful=True)), (FaithfulnessInput(response='Apple pies tastes bad.', retrieved_contexts=['An apple pie is a fruit pie in which the principal filling ingredient is apples.', "Apple pie is often served with whipped cream, ice cream ('apple pie  la mode'), custard or cheddar cheese.", 'It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).']), FaithfulnessOutput(faithful=False))], language=english)) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  faithfulness_prompt = MultiModalFaithfulnessPrompt(instruction=Please ...
     |
     |  name = 'faithful_rate'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class MultiModalRelevance(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  MultiModalRelevance(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'relevance_rate', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, relevance_prompt: 'ImageTextPrompt' = MultiModalRelevancePrompt(instruction=
     |  Your task is to evaluate if the response for the query is in line with the images and textual context information provided.
     |  You have two options to answer. Either True / False.
     |  Answer - True, if the response for the query is in line with context information otherwise False.
     |  , examples=[(RelevanceInput(user_input='What is the primary ingredient in a traditional Margherita pizza?', response='The primary ingredients in a Margherita pizza are tomatoes, mozzarella cheese, and fresh basil.', retrieved_contexts=['A traditional Margherita pizza consists of a thin crust.', 'The main toppings include tomatoes, mozzarella cheese, fresh basil, salt, and olive oil.', 'It is one of the simplest and most classic types of pizza.']), RelevanceOutput(relevance=True)), (RelevanceInput(user_input='Who won the Best Actor award at the Oscars in 2021?', response='The Best Actor award in 2021 was won by Leonardo DiCaprio.', retrieved_contexts=['The 93rd Academy Awards were held in 2021.', "Anthony Hopkins won the Best Actor award for his role in 'The Father'.", 'The event was unique due to COVID-19 restrictions.']), RelevanceOutput(relevance=False))], language=english)) -> None
     |
     |  MultiModalRelevance(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'relevance_rate', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, relevance_prompt: 'ImageTextPrompt' = MultiModalRelevancePrompt(instruction=
     |  Your task is to evaluate if the response for the query is in line with the images and textual context information provided.
     |  You have two options to answer. Either True / False.
     |  Answer - True, if the response for the query is in line with context information otherwise False.
     |  , examples=[(RelevanceInput(user_input='What is the primary ingredient in a traditional Margherita pizza?', response='The primary ingredients in a Margherita pizza are tomatoes, mozzarella cheese, and fresh basil.', retrieved_contexts=['A traditional Margherita pizza consists of a thin crust.', 'The main toppings include tomatoes, mozzarella cheese, fresh basil, salt, and olive oil.', 'It is one of the simplest and most classic types of pizza.']), RelevanceOutput(relevance=True)), (RelevanceInput(user_input='Who won the Best Actor award at the Oscars in 2021?', response='The Best Actor award in 2021 was won by Leonardo DiCaprio.', retrieved_contexts=['The 93rd Academy Awards were held in 2021.', "Anthony Hopkins won the Best Actor award for his role in 'The Father'.", 'The event was unique due to COVID-19 restrictions.']), RelevanceOutput(relevance=False))], language=english))
     |
     |  Method resolution order:
     |      MultiModalRelevance
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'relevance_rate', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, relevance_prompt: 'ImageTextPrompt' = MultiModalRelevancePrompt(instruction=
     |  Your task is to evaluate if the response for the query is in line with the images and textual context information provided.
     |  You have two options to answer. Either True / False.
     |  Answer - True, if the response for the query is in line with context information otherwise False.
     |  , examples=[(RelevanceInput(user_input='What is the primary ingredient in a traditional Margherita pizza?', response='The primary ingredients in a Margherita pizza are tomatoes, mozzarella cheese, and fresh basil.', retrieved_contexts=['A traditional Margherita pizza consists of a thin crust.', 'The main toppings include tomatoes, mozzarella cheese, fresh basil, salt, and olive oil.', 'It is one of the simplest and most classic types of pizza.']), RelevanceOutput(relevance=True)), (RelevanceInput(user_input='Who won the Best Actor award at the Oscars in 2021?', response='The Best Actor award in 2021 was won by Leonardo DiCaprio.', retrieved_contexts=['The 93rd Academy Awards were held in 2021.', "Anthony Hopkins won the Best Actor award for his role in 'The Father'.", 'The event was unique due to COVID-19 restrictions.']), RelevanceOutput(relevance=False))], language=english)) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  name = 'relevance_rate'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  relevance_prompt = MultiModalRelevancePrompt(instruction=
     |  Your task......
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class MultiTurnMetric(Metric)
     |  MultiTurnMetric(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '') -> None
     |
     |  A metric class for evaluating multi-turn conversations.
     |
     |  This class extends the base Metric class to provide functionality
     |  for scoring multi-turn conversation samples.
     |
     |  Method resolution order:
     |      MultiTurnMetric
     |      Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset({'_multi_turn_ascore', 'init'})
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Metric:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Metric:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name')
     |
     |  name = ''

    class NoiseSensitivity(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  NoiseSensitivity(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'noise_sensitivity', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['relevant', 'irrelevant']" = 'relevant', nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |
     |  NoiseSensitivity(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'noise_sensitivity', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['relevant', 'irrelevant']" = 'relevant', nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      NoiseSensitivity
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'noise_sensitivity', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['relevant', 'irrelevant']" = 'relevant', nli_statements_prompt: 'PydanticPrompt' = <factory>, statement_generator_prompt: 'PydanticPrompt' = <factory>, max_retries: 'int' = 1) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  max_retries = 1
     |
     |  mode = 'relevant'
     |
     |  name = 'noise_sensitivity'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class NonLLMContextPrecisionWithReference(ragas.metrics.base.SingleTurnMetric)
     |  NonLLMContextPrecisionWithReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'non_llm_context_precision_with_reference', distance_measure: 'SingleTurnMetric' = <factory>, threshold: 'float' = 0.5) -> None
     |
     |  NonLLMContextPrecisionWithReference(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'non_llm_context_precision_with_reference', distance_measure: 'SingleTurnMetric' = <factory>, threshold: 'float' = 0.5)
     |
     |  Method resolution order:
     |      NonLLMContextPrecisionWithReference
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'non_llm_context_precision_with_reference', distance_measure: 'SingleTurnMetric' = <factory>, threshold: 'float' = 0.5) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'distance_measure', 'th...
     |
     |  name = 'non_llm_context_precision_with_reference'
     |
     |  threshold = 0.5
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class NonLLMContextRecall(ragas.metrics.base.SingleTurnMetric)
     |  NonLLMContextRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'non_llm_context_recall', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>, _distance_measure: 'SingleTurnMetric' = <factory>, threshold: 'float' = 0.5) -> None
     |
     |  NonLLMContextRecall(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'non_llm_context_recall', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>, _distance_measure: 'SingleTurnMetric' = <factory>, threshold: 'float' = 0.5)
     |
     |  Method resolution order:
     |      NonLLMContextRecall
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'non_llm_context_recall', output_type: 'MetricOutputType' = <MetricOutputType.CONTINUOUS: 'continuous'>, _distance_measure: 'SingleTurnMetric' = <factory>, threshold: 'float' = 0.5) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  distance_measure
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_distance_measure': 'SingleTurnMetric', '_required...
     |
     |  __dataclass_fields__ = {'_distance_measure': Field(name='_distance_mea...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'output_type', '_distan...
     |
     |  name = 'non_llm_context_recall'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  threshold = 0.5
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class NonLLMStringSimilarity(ragas.metrics.base.SingleTurnMetric)
     |  NonLLMStringSimilarity(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'non_llm_string_similarity', distance_measure: ragas.metrics._string.DistanceMeasure = <DistanceMeasure.LEVENSHTEIN: 'levenshtein'>) -> None
     |
     |  NonLLMStringSimilarity(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'non_llm_string_similarity', distance_measure: ragas.metrics._string.DistanceMeasure = <DistanceMeasure.LEVENSHTEIN: 'levenshtein'>)
     |
     |  Method resolution order:
     |      NonLLMStringSimilarity
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'non_llm_string_similarity', distance_measure: ragas.metrics._string.DistanceMeasure = <DistanceMeasure.LEVENSHTEIN: 'levenshtein'>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'distance_measure')
     |
     |  distance_measure = <DistanceMeasure.LEVENSHTEIN: 'levenshtein'>
     |
     |  name = 'non_llm_string_similarity'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class NumericMetric(ragas.metrics.base.SimpleLLMMetric, ragas.metrics.validators.NumericValidator)
     |  NumericMetric(name: 'str', allowed_values: Union[Tuple[float, float], range] = (0.0, 1.0), prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |
     |  Metric for continuous numeric evaluations within a specified range.
     |
     |  This class is used for metrics that output numeric scores within a
     |  defined range, such as 0.0 to 1.0 for similarity scores or 1-10 ratings.
     |  Uses the instructor library for structured LLM outputs.
     |
     |  Attributes
     |  ----------
     |  allowed_values : Union[Tuple[float, float], range]
     |      The valid range for metric outputs. Can be a tuple of (min, max) floats
     |      or a range object. Default is (0.0, 1.0).
     |  llm : Optional[BaseRagasLLM]
     |      The language model instance for evaluation. Can be created using llm_factory().
     |  prompt : Optional[Union[str, Prompt]]
     |      The prompt template for the metric. Should contain placeholders for
     |      evaluation inputs that will be formatted at runtime.
     |
     |  Examples
     |  --------
     |  >>> from ragas.metrics import NumericMetric
     |  >>> from ragas.llms import llm_factory
     |  >>> from openai import OpenAI
     |  >>>
     |  >>> # Create an LLM instance
     |  >>> client = OpenAI(api_key="your-api-key")
     |  >>> llm = llm_factory("gpt-4o-mini", client=client)
     |  >>>
     |  >>> # Create a custom numeric metric with 0-10 range
     |  >>> metric = NumericMetric(
     |  ...     name="quality_score",
     |  ...     llm=llm,
     |  ...     prompt="Rate the quality of this response on a scale of 0-10: {response}",
     |  ...     allowed_values=(0.0, 10.0)
     |  ... )
     |  >>>
     |  >>> # Score with the metric
     |  >>> result = metric.score(
     |  ...     llm=llm,
     |  ...     response="This is a great response!"
     |  ... )
     |  >>> print(result.value)  # Output: a float between 0.0 and 10.0
     |
     |  Method resolution order:
     |      NumericMetric
     |      ragas.metrics.base.SimpleLLMMetric
     |      ragas.metrics.base.SimpleBaseMetric
     |      ragas.metrics.validators.NumericValidator
     |      ragas.metrics.validators.BaseValidator
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, name: 'str', allowed_values: Union[Tuple[float, float], range] = (0.0, 1.0), prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  get_correlation(self, gold_labels: List[str], predictions: List[str]) -> float
     |      Calculate the correlation between gold labels and predictions.
     |      This is a placeholder method and should be implemented based on the specific metric.
     |
     |  ----------------------------------------------------------------------
     |  Class methods defined here:
     |
     |  load(path: str, embedding_model: Optional[ForwardRef('EmbeddingModelType')] = None) -> 'NumericMetric'
     |      Load a NumericMetric from a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str
     |          File path to load from. Supports .gz compressed files.
     |      embedding_model : Optional[Any]
     |          Embedding model for DynamicFewShotPrompt. Required if the original used one.
     |
     |      Returns:
     |      --------
     |      NumericMetric
     |          Loaded metric instance
     |
     |      Raises:
     |      -------
     |      ValueError
     |          If file cannot be loaded or is not a NumericMetric
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'allowed_values': typing.Union[typing.Tuple[float, ...
     |
     |  __dataclass_fields__ = {'_response_model': Field(name='_response_model...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=False,eq=True,o...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('name', 'allowed_values', 'prompt')
     |
     |  allowed_values = (0.0, 1.0)
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SimpleLLMMetric:
     |
     |  __repr__(self) -> 'str'
     |      Return a clean string representation of the metric.
     |
     |  async abatch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Asynchronously calculate scores for a batch of inputs in parallel.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  align(self, train_dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          train_dataset: train_dataset to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |
     |      Align the metric with the specified experiments by different optimization methods.
     |
     |  align_and_validate(self, dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", llm: "'BaseRagasLLM'", test_size: 'float' = 0.2, random_state: 'int' = 42, **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          dataset: experiment to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |          llm: The LLM instance to use for scoring.
     |
     |      Align the metric with the specified experiments and validate it against a gold standard experiment.
     |      This method combines alignment and validation into a single step.
     |
     |  async ascore(self, **kwargs) -> "'MetricResult'"
     |      Asynchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  batch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Synchronously calculate scores for a batch of inputs.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  get_variables(self) -> 't.List[str]'
     |
     |  save(self, path: 't.Optional[str]' = None) -> 'None'
     |      Save the metric configuration to a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str, optional
     |          File path to save to. If not provided, saves to "./{metric.name}.json"
     |          Use .gz extension for compression.
     |
     |      Note:
     |      -----
     |      If the metric has a response_model, its schema will be saved for reference
     |      but the model itself cannot be serialized. You'll need to provide it when loading.
     |
     |      Examples:
     |      ---------
     |      All these work:
     |      >>> metric.save()                      # \u2192 ./response_quality.json
     |      >>> metric.save("custom.json")         # \u2192 ./custom.json
     |      >>> metric.save("/path/to/metrics/")   # \u2192 /path/to/metrics/response_quality.json
     |      >>> metric.save("no_extension")        # \u2192 ./no_extension.json
     |      >>> metric.save("compressed.json.gz")  # \u2192 ./compressed.json.gz (compressed)
     |
     |  score(self, **kwargs) -> "'MetricResult'"
     |      Synchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  validate_alignment(self, llm: "'BaseRagasLLM'", test_dataset: "'Dataset'", mapping: 't.Dict[str, str]' = {})
     |      Args:
     |          llm: The LLM instance to use for scoring.
     |          test_dataset: An Dataset instance containing the gold standard scores.
     |          mapping: A dictionary mapping variable names expected by metrics to their corresponding names in the gold experiment.
     |
     |      Validate the alignment of the metric by comparing the scores against a gold standard experiment.
     |      This method computes the Cohen's Kappa score and agreement rate between the gold standard scores and
     |      the predicted scores from the metric.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.SimpleLLMMetric:
     |
     |  prompt = None
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.SimpleBaseMetric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.validators.NumericValidator:
     |
     |  validate_result_value(self, result_value: Any) -> Optional[str]
     |      Validate that result value is within the numeric range.

    class RankingMetric(ragas.metrics.base.SimpleLLMMetric, ragas.metrics.validators.RankingValidator)
     |  RankingMetric(name: 'str', allowed_values: int = 2, prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |
     |  Metric for evaluations that produce ranked lists of items.
     |
     |  This class is used for metrics that output ordered lists, such as
     |  ranking search results, prioritizing features, or ordering responses
     |  by relevance. Uses the instructor library for structured LLM outputs.
     |
     |  Attributes
     |  ----------
     |  allowed_values : int
     |      Expected number of items in the ranking list. Default is 2.
     |  llm : Optional[BaseRagasLLM]
     |      The language model instance for evaluation. Can be created using llm_factory().
     |  prompt : Optional[Union[str, Prompt]]
     |      The prompt template for the metric. Should contain placeholders for
     |      evaluation inputs that will be formatted at runtime.
     |
     |  Examples
     |  --------
     |  >>> from ragas.metrics import RankingMetric
     |  >>> from ragas.llms import llm_factory
     |  >>> from openai import OpenAI
     |  >>>
     |  >>> # Create an LLM instance
     |  >>> client = OpenAI(api_key="your-api-key")
     |  >>> llm = llm_factory("gpt-4o-mini", client=client)
     |  >>>
     |  >>> # Create a ranking metric that returns top 3 items
     |  >>> metric = RankingMetric(
     |  ...     name="relevance_ranking",
     |  ...     llm=llm,
     |  ...     prompt="Rank these results by relevance: {results}",
     |  ...     allowed_values=3
     |  ... )
     |  >>>
     |  >>> # Score with the metric
     |  >>> result = metric.score(
     |  ...     llm=llm,
     |  ...     results="result1, result2, result3"
     |  ... )
     |  >>> print(result.value)  # Output: a list of 3 ranked items
     |
     |  Method resolution order:
     |      RankingMetric
     |      ragas.metrics.base.SimpleLLMMetric
     |      ragas.metrics.base.SimpleBaseMetric
     |      ragas.metrics.validators.RankingValidator
     |      ragas.metrics.validators.BaseValidator
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, name: 'str', allowed_values: int = 2, prompt: "t.Optional[t.Union[str, 'Prompt']]" = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  get_correlation(self, gold_labels: List[str], predictions: List[str]) -> float
     |      Calculate the correlation between gold labels and predictions.
     |      This is a placeholder method and should be implemented based on the specific metric.
     |
     |  ----------------------------------------------------------------------
     |  Class methods defined here:
     |
     |  load(path: str, embedding_model: Optional[ForwardRef('EmbeddingModelType')] = None) -> 'RankingMetric'
     |      Load a RankingMetric from a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str
     |          File path to load from. Supports .gz compressed files.
     |      embedding_model : Optional[Any]
     |          Embedding model for DynamicFewShotPrompt. Required if the original used one.
     |
     |      Returns:
     |      --------
     |      RankingMetric
     |          Loaded metric instance
     |
     |      Raises:
     |      -------
     |      ValueError
     |          If file cannot be loaded or is not a RankingMetric
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'allowed_values': <class 'int'>}
     |
     |  __dataclass_fields__ = {'_response_model': Field(name='_response_model...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=False,eq=True,o...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('name', 'allowed_values', 'prompt')
     |
     |  allowed_values = 2
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SimpleLLMMetric:
     |
     |  __repr__(self) -> 'str'
     |      Return a clean string representation of the metric.
     |
     |  async abatch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Asynchronously calculate scores for a batch of inputs in parallel.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  align(self, train_dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          train_dataset: train_dataset to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |
     |      Align the metric with the specified experiments by different optimization methods.
     |
     |  align_and_validate(self, dataset: "'Dataset'", embedding_model: "'EmbeddingModelType'", llm: "'BaseRagasLLM'", test_size: 'float' = 0.2, random_state: 'int' = 42, **kwargs: 't.Dict[str, t.Any]')
     |      Args:
     |          dataset: experiment to align the metric with.
     |          embedding_model: The embedding model used for dynamic few-shot prompting.
     |          llm: The LLM instance to use for scoring.
     |
     |      Align the metric with the specified experiments and validate it against a gold standard experiment.
     |      This method combines alignment and validation into a single step.
     |
     |  async ascore(self, **kwargs) -> "'MetricResult'"
     |      Asynchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  batch_score(self, inputs: 't.List[t.Dict[str, t.Any]]', **kwargs) -> "t.List['MetricResult']"
     |      Synchronously calculate scores for a batch of inputs.
     |
     |      Parameters
     |      ----------
     |      inputs : List[Dict[str, Any]]
     |          List of input dictionaries, each containing parameters for the metric.
     |
     |      Returns
     |      -------
     |      List[MetricResult]
     |          List of evaluation results, one for each input.
     |
     |  get_variables(self) -> 't.List[str]'
     |
     |  save(self, path: 't.Optional[str]' = None) -> 'None'
     |      Save the metric configuration to a JSON file.
     |
     |      Parameters:
     |      -----------
     |      path : str, optional
     |          File path to save to. If not provided, saves to "./{metric.name}.json"
     |          Use .gz extension for compression.
     |
     |      Note:
     |      -----
     |      If the metric has a response_model, its schema will be saved for reference
     |      but the model itself cannot be serialized. You'll need to provide it when loading.
     |
     |      Examples:
     |      ---------
     |      All these work:
     |      >>> metric.save()                      # \u2192 ./response_quality.json
     |      >>> metric.save("custom.json")         # \u2192 ./custom.json
     |      >>> metric.save("/path/to/metrics/")   # \u2192 /path/to/metrics/response_quality.json
     |      >>> metric.save("no_extension")        # \u2192 ./no_extension.json
     |      >>> metric.save("compressed.json.gz")  # \u2192 ./compressed.json.gz (compressed)
     |
     |  score(self, **kwargs) -> "'MetricResult'"
     |      Synchronously calculate the metric score.
     |
     |      Parameters
     |      ----------
     |      **kwargs : dict
     |          Input parameters required by the specific metric implementation.
     |
     |      Returns
     |      -------
     |      MetricResult
     |          The evaluation result containing the score and reasoning.
     |
     |  validate_alignment(self, llm: "'BaseRagasLLM'", test_dataset: "'Dataset'", mapping: 't.Dict[str, str]' = {})
     |      Args:
     |          llm: The LLM instance to use for scoring.
     |          test_dataset: An Dataset instance containing the gold standard scores.
     |          mapping: A dictionary mapping variable names expected by metrics to their corresponding names in the gold experiment.
     |
     |      Validate the alignment of the metric by comparing the scores against a gold standard experiment.
     |      This method computes the Cohen's Kappa score and agreement rate between the gold standard scores and
     |      the predicted scores from the metric.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.SimpleLLMMetric:
     |
     |  prompt = None
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.SimpleBaseMetric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.validators.RankingValidator:
     |
     |  validate_result_value(self, result_value: Any) -> Optional[str]
     |      Validate that result value is a list with expected length.

    class ResponseGroundedness(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  ResponseGroundedness(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'nv_response_groundedness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |
     |  Parameters:
     |  Score the groundedness of the response based on the retrieved contexts.
     |
     |  Input:
     |      data: list of Dicts with keys: response, retrieved contexts
     |  Output:
     |      0.0: response is not grounded in the retrieved contexts
     |      0.5: response is partially grounded in the retrieved contexts
     |      1.0: response is fully grounded in the retrieved contexts
     |
     |  Method resolution order:
     |      ResponseGroundedness
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'nv_response_groundedness', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  average_scores(self, score0, score1)
     |
     |  process_score(self, response)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  name = 'nv_response_groundedness'
     |
     |  retry = 5
     |
     |  template_groundedness1 = '### Instruction\n\nYou are a world class exp...
     |
     |  template_groundedness2 = 'As a specialist in assessing the strength of...
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class ResponseRelevancy(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MetricWithEmbeddings, ragas.metrics.base.SingleTurnMetric)
     |  ResponseRelevancy(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_relevancy', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, question_generation: 'PydanticPrompt' = ResponseRelevancePrompt(instruction=Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, "I don't know" or "I'm not sure" are noncommittal answers, examples=[(ResponseRelevanceInput(response='Albert Einstein was born in Germany.'), ResponseRelevanceOutput(question='Where was Albert Einstein born?', noncommittal=0)), (ResponseRelevanceInput(response="I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. "), ResponseRelevanceOutput(question='What was the groundbreaking feature of the smartphone invented in 2023?', noncommittal=1))], language=english), strictness: 'int' = 3) -> None
     |
     |  Scores the relevancy of the answer according to the given question.
     |  Answers with incomplete, redundant or unnecessary information is penalized.
     |  Score can range from 0 to 1 with 1 being the best.
     |
     |  Attributes
     |  ----------
     |  name: string
     |      The name of the metrics
     |  strictness: int
     |      Here indicates the number questions generated per answer.
     |      Ideal range between 3 to 5.
     |  embeddings: Embedding
     |      The langchain wrapper of Embedding object.
     |      E.g. HuggingFaceEmbeddings('BAAI/bge-base-en')
     |
     |  Method resolution order:
     |      ResponseRelevancy
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.MetricWithEmbeddings
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'answer_relevancy', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = None, question_generation: 'PydanticPrompt' = ResponseRelevancePrompt(instruction=Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, "I don't know" or "I'm not sure" are noncommittal answers, examples=[(ResponseRelevanceInput(response='Albert Einstein was born in Germany.'), ResponseRelevanceOutput(question='Where was Albert Einstein born?', noncommittal=0)), (ResponseRelevanceInput(response="I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. "), ResponseRelevanceOutput(question='What was the groundbreaking feature of the smartphone invented in 2023?', noncommittal=1))], language=english), strictness: 'int' = 3) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  calculate_similarity(self, question: 'str', generated_questions: 'list[str]')
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'embeddings', 'llm', 'o...
     |
     |  name = 'answer_relevancy'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  question_generation = ResponseRelevancePrompt(instruction=Generate a q...
     |
     |  strictness = 3
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  embeddings = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class RougeScore(ragas.metrics.base.SingleTurnMetric)
     |  RougeScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'rouge_score', rouge_type: Literal['rouge1', 'rougeL'] = 'rougeL', mode: Literal['fmeasure', 'precision', 'recall'] = 'fmeasure') -> None
     |
     |  RougeScore(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'rouge_score', rouge_type: Literal['rouge1', 'rougeL'] = 'rougeL', mode: Literal['fmeasure', 'precision', 'recall'] = 'fmeasure')
     |
     |  Method resolution order:
     |      RougeScore
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'rouge_score', rouge_type: Literal['rouge1', 'rougeL'] = 'rougeL', mode: Literal['fmeasure', 'precision', 'recall'] = 'fmeasure') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'rouge_type', 'mode')
     |
     |  mode = 'fmeasure'
     |
     |  name = 'rouge_score'
     |
     |  rouge_type = 'rougeL'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class RubricsScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
     |  RubricsScore(name: 'str' = 'domain_specific_rubrics', rubrics: 't.Dict[str, str]' = {'score1_description': 'The response is entirely incorrect and fails to address any aspect of the user input.', 'score2_description': 'The response contains partial accuracy but includes major errors or significant omissions that affect its relevance to the user input.', 'score3_description': 'The response is mostly accurate but lacks clarity, thoroughness, or minor details needed to fully address the user input.', 'score4_description': 'The response is accurate and clear, with only minor omissions or slight inaccuracies in addressing the user input.', 'score5_description': 'The response is completely accurate, clear, and thoroughly addresses the user input without any errors or omissions.'}, llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.DISCRETE: 'discrete'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, max_retries: 'int' = 1)
     |
     |  Method resolution order:
     |      RubricsScore
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, name: 'str' = 'domain_specific_rubrics', rubrics: 't.Dict[str, str]' = {'score1_description': 'The response is entirely incorrect and fails to address any aspect of the user input.', 'score2_description': 'The response contains partial accuracy but includes major errors or significant omissions that affect its relevance to the user input.', 'score3_description': 'The response is mostly accurate but lacks clarity, thoroughness, or minor details needed to fully address the user input.', 'score4_description': 'The response is accurate and clear, with only minor omissions or slight inaccuracies in addressing the user input.', 'score5_description': 'The response is completely accurate, clear, and thoroughly addresses the user input without any errors or omissions.'}, llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.DISCRETE: 'discrete'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, max_retries: 'int' = 1)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self) -> 'str'
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.Metric:
     |
     |  name = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class SemanticSimilarity(ragas.metrics.base.MetricWithEmbeddings, ragas.metrics.base.SingleTurnMetric)
     |  SemanticSimilarity(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'semantic_similarity', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, is_cross_encoder: 'bool' = False, threshold: 't.Optional[float]' = None) -> None
     |
     |  Scores the semantic similarity of ground truth with generated answer.
     |  cross encoder score is used to quantify semantic similarity.
     |  SAS paper: https://arxiv.org/pdf/2108.06130.pdf
     |
     |  Attributes
     |  ----------
     |  name : str
     |  model_name:
     |      The model to be used for calculating semantic similarity
     |      Defaults open-ai-embeddings
     |      select cross-encoder model for best results
     |      https://huggingface.co/spaces/mteb/leaderboard
     |  threshold:
     |      The threshold if given used to map output to binary
     |      Default 0.5
     |
     |  Method resolution order:
     |      SemanticSimilarity
     |      ragas.metrics.base.MetricWithEmbeddings
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'semantic_similarity', embeddings: 't.Optional[t.Union[BaseRagasEmbeddings, BaseRagasEmbedding]]' = None, is_cross_encoder: 'bool' = False, threshold: 't.Optional[float]' = None) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'embeddings', 'is_cross...
     |
     |  is_cross_encoder = False
     |
     |  name = 'semantic_similarity'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  threshold = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  init(self, run_config: 'RunConfig')
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithEmbeddings:
     |
     |  embeddings = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class SimpleCriteriaScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric, ragas.metrics.base.MultiTurnMetric)
     |  SimpleCriteriaScore(name: 'str', definition: 'str', llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.DISCRETE: 'discrete'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, strictness: 'int' = 1)
     |
     |  Judges the submission to give binary results using the criteria specified
     |  in the metric definition.
     |
     |  Attributes
     |  ----------
     |  name: str
     |      name of the metrics
     |  definition: str
     |      criteria to score the submission
     |  strictness: int
     |      The number of times self consistency checks is made. Final judgement is
     |      made using majority vote.
     |
     |  Method resolution order:
     |      SimpleCriteriaScore
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, name: 'str', definition: 'str', llm: 't.Optional[BaseRagasLLM]' = None, required_columns: 't.Optional[t.Dict[MetricType, t.Set[str]]]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.DISCRETE: 'discrete'>, single_turn_prompt: 't.Optional[PydanticPrompt]' = None, multi_turn_prompt: 't.Optional[PydanticPrompt]' = None, strictness: 'int' = 1)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self) -> 'str'
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  definition
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type')
     |
     |  llm = None
     |
     |  output_type = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.Metric:
     |
     |  name = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class SingleTurnMetric(Metric)
     |  SingleTurnMetric(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '') -> None
     |
     |  A metric class for evaluating single-turn interactions.
     |
     |  This class provides methods to score single-turn samples, both synchronously and asynchronously.
     |
     |  Method resolution order:
     |      SingleTurnMetric
     |      Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset({'_single_turn_ascore', 'init'})
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Metric:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = '') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __post_init__(self)
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Metric:
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name')
     |
     |  name = ''

    class StringPresence(ragas.metrics.base.SingleTurnMetric)
     |  StringPresence(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'string_present') -> None
     |
     |  StringPresence(_required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'string_present')
     |
     |  Method resolution order:
     |      StringPresence
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: Dict[ragas.metrics.base.MetricType, Set[str]] = <factory>, name: str = 'string_present') -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config: ragas.run_config.RunConfig)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': typing.Dict[ragas.metrics.base...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name')
     |
     |  name = 'string_present'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class SummarizationScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.SingleTurnMetric)
     |  SummarizationScore(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'summary_score', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, max_retries: 'int' = 1, length_penalty: 'bool' = True, coeff: 'float' = 0.5, question_generation_prompt: 'PydanticPrompt' = <factory>, answer_generation_prompt: 'PydanticPrompt' = <factory>, extract_keyphrases_prompt: 'PydanticPrompt' = <factory>) -> None
     |
     |  SummarizationScore(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'summary_score', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, max_retries: 'int' = 1, length_penalty: 'bool' = True, coeff: 'float' = 0.5, question_generation_prompt: 'PydanticPrompt' = <factory>, answer_generation_prompt: 'PydanticPrompt' = <factory>, extract_keyphrases_prompt: 'PydanticPrompt' = <factory>)
     |
     |  Method resolution order:
     |      SummarizationScore
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.SingleTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'summary_score', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, max_retries: 'int' = 1, length_penalty: 'bool' = True, coeff: 'float' = 0.5, question_generation_prompt: 'PydanticPrompt' = <factory>, answer_generation_prompt: 'PydanticPrompt' = <factory>, extract_keyphrases_prompt: 'PydanticPrompt' = <factory>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  coeff = 0.5
     |
     |  length_penalty = True
     |
     |  max_retries = 1
     |
     |  name = 'summary_score'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.SingleTurnMetric:
     |
     |  async single_turn_ascore(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Asynchronously score a single-turn sample with an optional timeout.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  single_turn_score(self, sample: 'SingleTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Synchronously score a single-turn sample.
     |
     |      May raise ImportError if nest_asyncio is not installed in a Jupyter-like environment.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

    class ToolCallAccuracy(ragas.metrics.base.MultiTurnMetric)
     |  ToolCallAccuracy(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'tool_call_accuracy', strict_order: 'bool' = True, arg_comparison_metric: 'SingleTurnMetric' = <factory>) -> None
     |
     |  Tool Call Accuracy metric measures how accurately an LLM agent makes tool calls
     |  compared to reference tool calls.
     |
     |  The metric supports two evaluation modes:
     |  1. Strict order (default): Tool calls must match exactly in sequence
     |  2. Flexible order: Tool calls can be in any order (parallel evaluation)
     |
     |  The metric evaluates two aspects:
     |  1. Sequence alignment: Whether predicted and reference tool calls match in the required order
     |  2. Argument accuracy: How well tool call arguments match between predicted and reference
     |
     |  Score calculation:
     |  - If sequences don't align: score = 0
     |  - If sequences align: score = (average argument accuracy) * sequence_alignment_factor
     |  - Length mismatches result in warnings and proportional penalty
     |
     |  Edge cases:
     |  - No predicted tool calls: returns 0.0
     |  - Length mismatch: compares only the overlapping portion and applies coverage penalty
     |  - Missing arguments: contributes 0 to the argument score for that tool call
     |
     |  The final score is always between 0.0 and 1.0.
     |
     |  Args:
     |      strict_order: If True (default), tool calls must match exactly in sequence.
     |                   If False, tool calls can be in any order (parallel evaluation).
     |
     |  Method resolution order:
     |      ToolCallAccuracy
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'tool_call_accuracy', strict_order: 'bool' = True, arg_comparison_metric: 'SingleTurnMetric' = <factory>) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  is_sequence_aligned(self, pred_sequence: 't.List[str]', ref_sequence: 't.List[str]') -> 'bool'
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'strict_order', 'arg_co...
     |
     |  name = 'tool_call_accuracy'
     |
     |  strict_order = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class ToolCallF1(ragas.metrics.base.MultiTurnMetric)
     |  ToolCallF1(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'tool_call_f1', batch_size: 'int' = 1, is_multi_turn: 'bool' = True) -> None
     |
     |  ToolCallF1(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'tool_call_f1', batch_size: 'int' = 1, is_multi_turn: 'bool' = True)
     |
     |  Method resolution order:
     |      ToolCallF1
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'tool_call_f1', batch_size: 'int' = 1, is_multi_turn: 'bool' = True) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  init(self, run_config)
     |      Initialize the metric with the given run configuration.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run including timeouts and other settings.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'batch_size', 'is_multi...
     |
     |  batch_size = 1
     |
     |  is_multi_turn = True
     |
     |  name = 'tool_call_f1'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns

    class TopicAdherenceScore(ragas.metrics.base.MetricWithLLM, ragas.metrics.base.MultiTurnMetric)
     |  TopicAdherenceScore(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'topic_adherence', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['precision', 'recall', 'f1']" = 'f1', topic_extraction_prompt: 'PydanticPrompt' = TopicExtractionPrompt(instruction=Given an interaction between Human, Tool and AI, extract the topics from Human's input., examples=[(TopicExtractionInput(user_input='Human: Can you provide me with details about Einstein\'s theory of relativity?\nAI: Sure, let me retrieve the relevant information for you.\nTools:\n  document_search: {\'query\': "Einstein\'s theory of relativity"}\nToolOutput: Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\nAI: I found some documents on Einstein\'s theory of relativity. Which one would you like to know more about: \'Relativity: The Special and the General Theory\' or \'General Theory of Relativity by A. Einstein\'?\nHuman: Tell me about the \'General Theory of Relativity\'.\nAI: Got it! Let me fetch more details from \'General Theory of Relativity by A. Einstein\'.\nTools:\n  document_retrieve: {\'document\': \'General Theory of Relativity by A. Einstein\'}\nToolOutput: The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\nAI: The \'General Theory of Relativity\' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\nHuman: That\'s perfect, thank you!\nAI: You\'re welcome! Feel free to ask if you need more information.'), TopicExtractionOutput(topics=["Einstein's theory of relativity", 'General Theory of Relativity']))], language=english), topic_classification_prompt: 'PydanticPrompt' = TopicClassificationPrompt(instruction=Given a set of topics classify if the topic falls into any of the given reference topics., examples=[(TopicClassificationInput(reference_topics=['Physics', 'Mathematics'], topics=["Einstein's theory of relativity", 'General Theory of Relativity']), TopicClassificationOutput(classifications=[True, False]))], language=english), topic_refused_prompt: 'PydanticPrompt' = TopicRefusedPrompt(instruction=Given a topic, classify if the AI refused to answer the question about the topic., examples=[(TopicRefusedInput(user_input='Human: Can you provide me with details about Einstein\'s theory of relativity?\nAI: Sure, let me retrieve the relevant information for you.\nTools:\n  document_search: {\'query\': "Einstein\'s theory of relativity"}\nToolOutput: Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\nAI: I found some documents on Einstein\'s theory of relativity. Which one would you like to know more about: \'Relativity: The Special and the General Theory\' or \'General Theory of Relativity by A. Einstein\'?\nHuman: Tell me about the \'General Theory of Relativity\'.\nAI: Got it! Let me fetch more details from \'General Theory of Relativity by A. Einstein\'.\nTools:\n  document_retrieve: {\'document\': \'General Theory of Relativity by A. Einstein\'}\nToolOutput: The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\nAI: The \'General Theory of Relativity\' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\nHuman: That\'s perfect, thank you!\nAI: You\'re welcome! Feel free to ask if you need more information.', topic='General Theory of Relativity'), TopicRefusedOutput(refused_to_answer=False))], language=english)) -> None
     |
     |  TopicAdherenceScore(_required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'topic_adherence', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['precision', 'recall', 'f1']" = 'f1', topic_extraction_prompt: 'PydanticPrompt' = TopicExtractionPrompt(instruction=Given an interaction between Human, Tool and AI, extract the topics from Human's input., examples=[(TopicExtractionInput(user_input='Human: Can you provide me with details about Einstein\'s theory of relativity?\nAI: Sure, let me retrieve the relevant information for you.\nTools:\n  document_search: {\'query\': "Einstein\'s theory of relativity"}\nToolOutput: Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\nAI: I found some documents on Einstein\'s theory of relativity. Which one would you like to know more about: \'Relativity: The Special and the General Theory\' or \'General Theory of Relativity by A. Einstein\'?\nHuman: Tell me about the \'General Theory of Relativity\'.\nAI: Got it! Let me fetch more details from \'General Theory of Relativity by A. Einstein\'.\nTools:\n  document_retrieve: {\'document\': \'General Theory of Relativity by A. Einstein\'}\nToolOutput: The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\nAI: The \'General Theory of Relativity\' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\nHuman: That\'s perfect, thank you!\nAI: You\'re welcome! Feel free to ask if you need more information.'), TopicExtractionOutput(topics=["Einstein's theory of relativity", 'General Theory of Relativity']))], language=english), topic_classification_prompt: 'PydanticPrompt' = TopicClassificationPrompt(instruction=Given a set of topics classify if the topic falls into any of the given reference topics., examples=[(TopicClassificationInput(reference_topics=['Physics', 'Mathematics'], topics=["Einstein's theory of relativity", 'General Theory of Relativity']), TopicClassificationOutput(classifications=[True, False]))], language=english), topic_refused_prompt: 'PydanticPrompt' = TopicRefusedPrompt(instruction=Given a topic, classify if the AI refused to answer the question about the topic., examples=[(TopicRefusedInput(user_input='Human: Can you provide me with details about Einstein\'s theory of relativity?\nAI: Sure, let me retrieve the relevant information for you.\nTools:\n  document_search: {\'query\': "Einstein\'s theory of relativity"}\nToolOutput: Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\nAI: I found some documents on Einstein\'s theory of relativity. Which one would you like to know more about: \'Relativity: The Special and the General Theory\' or \'General Theory of Relativity by A. Einstein\'?\nHuman: Tell me about the \'General Theory of Relativity\'.\nAI: Got it! Let me fetch more details from \'General Theory of Relativity by A. Einstein\'.\nTools:\n  document_retrieve: {\'document\': \'General Theory of Relativity by A. Einstein\'}\nToolOutput: The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\nAI: The \'General Theory of Relativity\' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\nHuman: That\'s perfect, thank you!\nAI: You\'re welcome! Feel free to ask if you need more information.', topic='General Theory of Relativity'), TopicRefusedOutput(refused_to_answer=False))], language=english))
     |
     |  Method resolution order:
     |      TopicAdherenceScore
     |      ragas.metrics.base.MetricWithLLM
     |      ragas.metrics.base.MultiTurnMetric
     |      ragas.metrics.base.Metric
     |      abc.ABC
     |      ragas.prompt.mixin.PromptMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __eq__(self, other)
     |      Return self==value.
     |
     |  __init__(self, _required_columns: 't.Dict[MetricType, t.Set[str]]' = <factory>, name: 'str' = 'topic_adherence', llm: 't.Optional[BaseRagasLLM]' = None, output_type: 't.Optional[MetricOutputType]' = <MetricOutputType.CONTINUOUS: 'continuous'>, mode: "t.Literal['precision', 'recall', 'f1']" = 'f1', topic_extraction_prompt: 'PydanticPrompt' = TopicExtractionPrompt(instruction=Given an interaction between Human, Tool and AI, extract the topics from Human's input., examples=[(TopicExtractionInput(user_input='Human: Can you provide me with details about Einstein\'s theory of relativity?\nAI: Sure, let me retrieve the relevant information for you.\nTools:\n  document_search: {\'query\': "Einstein\'s theory of relativity"}\nToolOutput: Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\nAI: I found some documents on Einstein\'s theory of relativity. Which one would you like to know more about: \'Relativity: The Special and the General Theory\' or \'General Theory of Relativity by A. Einstein\'?\nHuman: Tell me about the \'General Theory of Relativity\'.\nAI: Got it! Let me fetch more details from \'General Theory of Relativity by A. Einstein\'.\nTools:\n  document_retrieve: {\'document\': \'General Theory of Relativity by A. Einstein\'}\nToolOutput: The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\nAI: The \'General Theory of Relativity\' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\nHuman: That\'s perfect, thank you!\nAI: You\'re welcome! Feel free to ask if you need more information.'), TopicExtractionOutput(topics=["Einstein's theory of relativity", 'General Theory of Relativity']))], language=english), topic_classification_prompt: 'PydanticPrompt' = TopicClassificationPrompt(instruction=Given a set of topics classify if the topic falls into any of the given reference topics., examples=[(TopicClassificationInput(reference_topics=['Physics', 'Mathematics'], topics=["Einstein's theory of relativity", 'General Theory of Relativity']), TopicClassificationOutput(classifications=[True, False]))], language=english), topic_refused_prompt: 'PydanticPrompt' = TopicRefusedPrompt(instruction=Given a topic, classify if the AI refused to answer the question about the topic., examples=[(TopicRefusedInput(user_input='Human: Can you provide me with details about Einstein\'s theory of relativity?\nAI: Sure, let me retrieve the relevant information for you.\nTools:\n  document_search: {\'query\': "Einstein\'s theory of relativity"}\nToolOutput: Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\nAI: I found some documents on Einstein\'s theory of relativity. Which one would you like to know more about: \'Relativity: The Special and the General Theory\' or \'General Theory of Relativity by A. Einstein\'?\nHuman: Tell me about the \'General Theory of Relativity\'.\nAI: Got it! Let me fetch more details from \'General Theory of Relativity by A. Einstein\'.\nTools:\n  document_retrieve: {\'document\': \'General Theory of Relativity by A. Einstein\'}\nToolOutput: The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\nAI: The \'General Theory of Relativity\' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\nHuman: That\'s perfect, thank you!\nAI: You\'re welcome! Feel free to ask if you need more information.', topic='General Theory of Relativity'), TopicRefusedOutput(refused_to_answer=False))], language=english)) -> None
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __abstractmethods__ = frozenset()
     |
     |  __annotations__ = {'_required_columns': 't.Dict[MetricType, t.Set[str]...
     |
     |  __dataclass_fields__ = {'_required_columns': Field(name='_required_col...
     |
     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...
     |
     |  __hash__ = None
     |
     |  __match_args__ = ('_required_columns', 'name', 'llm', 'output_type', '...
     |
     |  mode = 'f1'
     |
     |  name = 'topic_adherence'
     |
     |  output_type = <MetricOutputType.CONTINUOUS: 'continuous'>
     |
     |  topic_classification_prompt = TopicClassificationPrompt(instruction=Gi...
     |
     |  topic_extraction_prompt = TopicExtractionPrompt(instruction=Given an i...
     |
     |  topic_refused_prompt = TopicRefusedPrompt(instruction=Given a topic, c...
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  init(self, run_config: 'RunConfig') -> 'None'
     |      Initialize the metric with run configuration and validate LLM is present.
     |
     |      Parameters
     |      ----------
     |      run_config : RunConfig
     |          Configuration for the metric run.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If no LLM is provided to the metric.
     |
     |  train(self, path: 'str', demonstration_config: 't.Optional[DemonstrationConfig]' = None, instruction_config: 't.Optional[InstructionConfig]' = None, callbacks: 't.Optional[Callbacks]' = None, run_config: 't.Optional[RunConfig]' = None, batch_size: 't.Optional[int]' = None, with_debugging_logs=False, raise_exceptions: 'bool' = True) -> 'None'
     |      Train the metric using local JSON data
     |
     |      Parameters
     |      ----------
     |      path : str
     |          Path to local JSON training data file
     |      demonstration_config : DemonstrationConfig, optional
     |          Configuration for demonstration optimization
     |      instruction_config : InstructionConfig, optional
     |          Configuration for instruction optimization
     |      callbacks : Callbacks, optional
     |          List of callback functions
     |      run_config : RunConfig, optional
     |          Run configuration
     |      batch_size : int, optional
     |          Batch size for training
     |      with_debugging_logs : bool, default=False
     |          Enable debugging logs
     |      raise_exceptions : bool, default=True
     |          Whether to raise exceptions during training
     |
     |      Raises
     |      ------
     |      ValueError
     |          If path is not provided or not a JSON file
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from ragas.metrics.base.MetricWithLLM:
     |
     |  llm = None
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.MultiTurnMetric:
     |
     |  async multi_turn_ascore(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None, timeout: 't.Optional[float]' = None) -> 'float'
     |      Score a multi-turn conversation sample asynchronously.
     |
     |      May raise asyncio.TimeoutError if the scoring process exceeds the specified timeout.
     |
     |  multi_turn_score(self, sample: 'MultiTurnSample', callbacks: 'Callbacks' = None) -> 'float'
     |      Score a multi-turn conversation sample synchronously.
     |
     |      May raise ImportError if nest_asyncio is not installed in Jupyter-like environments.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.metrics.base.Metric:
     |
     |  __post_init__(self)
     |
     |  get_required_columns(self, with_optional: 'bool' = False) -> 't.Dict[str, t.Set[str]]'
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from ragas.metrics.base.Metric:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  required_columns
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from ragas.prompt.mixin.PromptMixin:
     |
     |  async adapt_prompts(self, language: 'str', llm: 't.Union[BaseRagasLLM, InstructorBaseRagasLLM]', adapt_instruction: 'bool' = False) -> 't.Dict[str, PydanticPrompt]'
     |      Adapts the prompts in the class to the given language and using the given LLM.
     |
     |      Notes
     |      -----
     |      Make sure you use the best available LLM for adapting the prompts and then save and load the prompts using
     |      [save_prompts][ragas.prompt.mixin.PromptMixin.save_prompts] and [load_prompts][ragas.prompt.mixin.PromptMixin.load_prompts]
     |      methods.
     |
     |  get_prompts(self) -> 't.Dict[str, PydanticPrompt]'
     |      Returns a dictionary of prompts for the class.
     |
     |  load_prompts(self, path: 'str', language: 't.Optional[str]' = None)
     |      Loads the prompts from a path. File should be in the format of {name}_{language}.json
     |
     |  save_prompts(self, path: 'str')
     |      Saves the prompts to a directory in the format of {name}_{language}.json
     |
     |  set_prompts(self, **prompts)
     |      Sets the prompts for the class.
     |
     |      Raises
     |      ------
     |      ValueError
     |          If the prompt is not an instance of `PydanticPrompt`.

FUNCTIONS
    discrete_metric(*, name: Optional[str] = None, allowed_values: Optional[List[str]] = None, **metric_params: Any) -> Callable[[Callable[..., Any]], ragas.metrics.decorator.DiscreteMetricProtocol]
        Decorator for creating discrete/categorical metrics.

        This decorator transforms a regular function into a DiscreteMetric instance
        that can be used for evaluation with predefined categorical outputs.

        Parameters
        ----------
        name : str, optional
            Name for the metric. If not provided, uses the function name.
        allowed_values : List[str], optional
            List of allowed categorical values for the metric output.
            Default is ["pass", "fail"].
        **metric_params : Any
            Additional parameters to pass to the metric initialization.

        Returns
        -------
        Callable[[Callable[..., Any]], DiscreteMetricProtocol]
            A decorator that transforms a function into a DiscreteMetric instance.

        Examples
        --------
        >>> from ragas.metrics import discrete_metric
        >>>
        >>> @discrete_metric(name="sentiment", allowed_values=["positive", "neutral", "negative"])
        >>> def sentiment_analysis(user_input: str, response: str) -> str:
        ...     '''Analyze sentiment of the response.'''
        ...     if "great" in response.lower() or "good" in response.lower():
        ...         return "positive"
        ...     elif "bad" in response.lower() or "poor" in response.lower():
        ...         return "negative"
        ...     return "neutral"
        >>>
        >>> result = sentiment_analysis(
        ...     user_input="How was your day?",
        ...     response="It was great!"
        ... )
        >>> print(result.value)  # "positive"

    numeric_metric(*, name: Optional[str] = None, allowed_values: Union[Tuple[float, float], range, NoneType] = None, **metric_params: Any) -> Callable[[Callable[..., Any]], ragas.metrics.decorator.NumericMetricProtocol]
        Decorator for creating numeric/continuous metrics.

        This decorator transforms a regular function into a NumericMetric instance
        that outputs continuous values within a specified range.

        Parameters
        ----------
        name : str, optional
            Name for the metric. If not provided, uses the function name.
        allowed_values : Union[Tuple[float, float], range], optional
            The valid range for metric outputs as (min, max) tuple or range object.
            Default is (0.0, 1.0).
        **metric_params : Any
            Additional parameters to pass to the metric initialization.

        Returns
        -------
        Callable[[Callable[..., Any]], NumericMetricProtocol]
            A decorator that transforms a function into a NumericMetric instance.

        Examples
        --------
        >>> from ragas.metrics import numeric_metric
        >>>
        >>> @numeric_metric(name="relevance_score", allowed_values=(0.0, 1.0))
        >>> def calculate_relevance(user_input: str, response: str) -> float:
        ...     '''Calculate relevance score between 0 and 1.'''
        ...     # Simple word overlap example
        ...     user_words = set(user_input.lower().split())
        ...     response_words = set(response.lower().split())
        ...     if not user_words:
        ...         return 0.0
        ...     overlap = len(user_words & response_words)
        ...     return overlap / len(user_words)
        >>>
        >>> result = calculate_relevance(
        ...     user_input="What is Python?",
        ...     response="Python is a programming language"
        ... )
        >>> print(result.value)  # Numeric score between 0.0 and 1.0

    ranking_metric(*, name: Optional[str] = None, allowed_values: Optional[int] = None, **metric_params: Any) -> Callable[[Callable[..., Any]], ragas.metrics.decorator.RankingMetricProtocol]
        Decorator for creating ranking/ordering metrics.

        This decorator transforms a regular function into a RankingMetric instance
        that outputs ordered lists of items.

        Parameters
        ----------
        name : str, optional
            Name for the metric. If not provided, uses the function name.
        allowed_values : int, optional
            Expected number of items in the ranking list. Default is 2.
        **metric_params : Any
            Additional parameters to pass to the metric initialization.

        Returns
        -------
        Callable[[Callable[..., Any]], RankingMetricProtocol]
            A decorator that transforms a function into a RankingMetric instance.

        Examples
        --------
        >>> from ragas.metrics import ranking_metric
        >>>
        >>> @ranking_metric(name="priority_ranker", allowed_values=3)
        >>> def rank_by_urgency(user_input: str, responses: list) -> list:
        ...     '''Rank responses by urgency keywords.'''
        ...     urgency_keywords = ["urgent", "asap", "critical"]
        ...     scored = []
        ...     for resp in responses:
        ...         score = sum(kw in resp.lower() for kw in urgency_keywords)
        ...         scored.append((score, resp))
        ...     # Sort by score descending and return top items
        ...     ranked = sorted(scored, key=lambda x: x[0], reverse=True)
        ...     return [item[1] for item in ranked[:3]]
        >>>
        >>> result = rank_by_urgency(
        ...     user_input="What should I do first?",
        ...     responses=["This is urgent", "Take your time", "Critical issue!"]
        ... )
        >>> print(result.value)  # Ranked list of responses

DATA
    __all__ = ['Metric', 'MetricType', 'MetricWithEmbeddings', 'MetricWith...
    answer_correctness = AnswerCorrectness(_required_columns={<MetricType....
    answer_relevancy = AnswerRelevancy(_required_columns={<MetricType.S......
    answer_similarity = AnswerSimilarity(_required_columns={<MetricType......
    context_entity_recall = ContextEntityRecall(_required_columns={<Metric...
    context_precision = ContextPrecision(_required_columns={<MetricType......
    context_recall = ContextRecall(_required_columns={<MetricType.SIN...ri...
    faithfulness = Faithfulness(_required_columns={<MetricType.SING...echa...
    multimodal_faithness = MultiModalFaithfulness(_required_columns={<Metr...
    multimodal_relevance = MultiModalRelevance(_required_columns={<MetricT...
    summarization_score = SummarizationScore(_required_columns={<MetricTyp...

FILE
    d:\programme\anaconda\envs\ragbench\lib\site-packages\ragas\metrics\__init__.py


