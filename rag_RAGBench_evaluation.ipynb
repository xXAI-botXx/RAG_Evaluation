{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f66db6e",
      "metadata": {
        "id": "8f66db6e"
      },
      "source": [
        "# RAG Evaluation with [RAGBench Benchmark](https://huggingface.co/datasets/galileo-ai/ragbench)\n",
        "\n",
        "A big dataset to evaluate your model with some evaluation scripts but no direct support for new custom model integrations. Therefore: No registration, installation (only HuggingFace) required.\n",
        "\n",
        "Used for evaluation of:\n",
        "- Hallucination Detection\n",
        "- Context Relevance Detection\n",
        "- Context Utilization Detection\n",
        "\n",
        "<br><br>\n",
        "\n",
        "- [System Setup](#system-setup)\n",
        "  - docker setup\n",
        "  - install RAGBench\n",
        "- [Example RAG Model](#example-rag-model)\n",
        "  - Retriever: Embedding + Indexing (Database) (+ example data)\n",
        "  - Reranker (we don't use one)\n",
        "  - Generator: Tokenizer + LLM\n",
        "- [Evaluation with RAGBench](#evaluation-with-ragbench)\n",
        "  - 1. Load Datasets\n",
        "  - 2. Evaluate your model\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0428195",
      "metadata": {},
      "source": [
        "### System Setup\n",
        "\n",
        "Using Docker\n",
        "\n",
        "You might want to check your CUDA version first:\n",
        "```bash\n",
        "!nvidia-smi | sed '/Processes/,$d'\n",
        "```\n",
        "\n",
        "- `nvidia-smi` -> standard NVIDIA information command\n",
        "- `|` -> send content to `sed` (which is a streaming editor)\n",
        "- `sed '/Processes/,$d'` -> delete (`d`) from line containing `Processes` to the end (`$`)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "> **Notice: Same setup as for BERGEN is used via Docker but this repository also would work with a much simpler system setup.**\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ceca329",
      "metadata": {},
      "source": [
        "**Build your image:**\n",
        "1. Open `Docker Desktop` and open the bash (right bottom corner)\n",
        "2. Run:\n",
        "    ```bash\n",
        "    cd D:\\Informatik\\Projekte\\RAG_Evaluation\n",
        "    docker build -t rag-eval .\n",
        "    ```\n",
        "\n",
        "**Starting Setup:**<br>\n",
        "1. Open `Docker Desktop`\n",
        "2. Starting Container:\n",
        "    ```bash\n",
        "    docker run -it --rm  --gpus all -v .:/workspace -w /workspace rag-eval bash\n",
        "    ```\n",
        "3. Attach Visual Studio to that Container (Docker Extension installation required)\n",
        "\n",
        "<br>\n",
        "\n",
        "[We used this Image](./Dockerfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f509379",
      "metadata": {},
      "source": [
        "### System Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7affa27",
      "metadata": {},
      "outputs": [],
      "source": [
        "import prime_printer as prime\n",
        "print(prime.get_hardware())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-CJwn31TDil",
      "metadata": {
        "id": "z-CJwn31TDil"
      },
      "source": [
        "### Example RAG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wGAmEayKTPSe",
      "metadata": {
        "id": "wGAmEayKTPSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u4Z2il9VTP-L",
      "metadata": {
        "id": "u4Z2il9VTP-L"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kVfIVA-OTV2T",
      "metadata": {
        "id": "kVfIVA-OTV2T"
      },
      "outputs": [],
      "source": [
        "example_documents = [\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The Pythagorean theorem describes the relationship between the sides of a right triangle.\",\n",
        "    \"The capital of Germany is Berlin.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab44002",
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(model, tokenizer, texts):\n",
        "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)  # CLS Token pooling\n",
        "        # attention_mask = tokens[\"attention_mask\"].unsqueeze(-1)\n",
        "        # embeddings = (outputs.last_hidden_state * attention_mask).sum(dim=1)\n",
        "        # embeddings = embeddings / attention_mask.sum(dim=1)\n",
        "    return embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QWkCxhy6TZS0",
      "metadata": {
        "id": "QWkCxhy6TZS0"
      },
      "outputs": [],
      "source": [
        "doc_embeddings = encode(embedding_model, tokenizer, example_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gyu2oxxCTzfn",
      "metadata": {
        "id": "Gyu2oxxCTzfn"
      },
      "source": [
        "Build FAISS Index (our \"database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfdpihG5TxDl",
      "metadata": {
        "id": "dfdpihG5TxDl"
      },
      "outputs": [],
      "source": [
        "dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "# save for later\n",
        "faiss.write_index(index, \"/content/my_index.faiss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gkGXrFoeT8BP",
      "metadata": {
        "id": "gkGXrFoeT8BP"
      },
      "source": [
        "Load a language model (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6kwAA6cTT8iw",
      "metadata": {
        "id": "6kwAA6cTT8iw"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"  # \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             torch_dtype=\"torch.float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KihX3WsSUGDD",
      "metadata": {
        "id": "KihX3WsSUGDD"
      },
      "source": [
        "RAG Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BBJ5TWimUIAk",
      "metadata": {
        "id": "BBJ5TWimUIAk"
      },
      "outputs": [],
      "source": [
        "def rag_answer(query, given_passages=None, k=2):\n",
        "    # Create prompt embedding\n",
        "    embedded_prompt = encode(embedding_model, tokenizer, [query])\n",
        "\n",
        "    # Retrieve top-k docs\n",
        "    if given_passages is None:\n",
        "        distances, idx = index.search(embedded_prompt, k)\n",
        "    else:\n",
        "        given_passages_embedded = encode(embedding_model, tokenizer, given_passages)\n",
        "        index = faiss.IndexFlatL2(dim)\n",
        "        index.add(given_passages_embedded)\n",
        "    retrieved = [example_documents[i] for i in idx[0]]\n",
        "\n",
        "    # Build the final prompt for generation\n",
        "    prompt = (\n",
        "        \"Use the following context to answer the given question.\\n\\n\"\n",
        "        f\"Context: {retrieved}\\n\\n\"\n",
        "        f\"Question: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode output\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True), retrieved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ZcStBgbU722",
      "metadata": {
        "id": "-ZcStBgbU722"
      },
      "source": [
        "Example Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ELwq7C49U-2-",
      "metadata": {
        "id": "ELwq7C49U-2-"
      },
      "outputs": [],
      "source": [
        "answer, retrieved_docs = rag_answer(\"Where is the Eiffel Tower located?\")\n",
        "print(\"Retrieved Docs:\", retrieved_docs)\n",
        "print(\"\\nRAG Answer:\\n\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xeDggQRSVM-P",
      "metadata": {
        "id": "xeDggQRSVM-P"
      },
      "source": [
        "### **Evaluation with RAGBench**\n",
        "\n",
        "Witht he given models it is easy:\n",
        "```bash\n",
        "python run_inference.py --dataset msmarco --model trulens --output results\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877cf20a",
      "metadata": {},
      "source": [
        "1. Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5852e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the full ragbench dataset delucionqa\n",
        "ragbench = {}\n",
        "columns = set()\n",
        "for dataset in ['covidqa', 'cuad', 'delusionqa', 'emanual', 'expertqa', 'finqa', 'hagrid', 'hotpotqa', 'msmarco', 'pubmedqa', 'tatqa', 'techqa']:\n",
        "  ragbench[dataset] = load_dataset(\"rungalileo/ragbench\", dataset)\n",
        "  print(f\"Loaded '{dataset}' dataset from RAGBench\")\n",
        "  columns = columns.union(set(ragbench[dataset]['test'].keys()))\n",
        "print(f\"Columns in ragbench datasets: {columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008fa747",
      "metadata": {},
      "source": [
        "2. Calculate scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb064eb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path += [\"./ragbench/ragbench\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55025374",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy\n",
        "\n",
        "def evaluate_rag_output(question, answer, contexts):\n",
        "    data = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"contexts\": contexts,\n",
        "    }\n",
        "\n",
        "    adherence = faithfulness(data)\n",
        "    relevance = context_relevancy(data)\n",
        "    utilization = answer_relevancy(data)\n",
        "\n",
        "    return float(adherence), float(relevance), float(utilization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d074f94d",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "for dataset_name, dataset in ragbench.items():\n",
        "    print(f\"Evaluating on {dataset_name}...\")\n",
        "    results[dataset_name] = []\n",
        "    for sample in dataset['test']:\n",
        "        question = sample['question']\n",
        "        # is there also context / documents given? FIXME\n",
        "        ground_truth = sample.get('answers', {}).get('text', [''])[0]  # Adjust based on dataset structure\n",
        "        \n",
        "        given_passages = sample.get('documents', [])\n",
        "        rag_response, contexts = rag_answer(question, given_passages)\n",
        "\n",
        "        adherence, relevance, utilization = evaluate_rag_output(question, answer, contexts)\n",
        "        \n",
        "        results[dataset_name] += [{\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'rag_response': rag_response,\n",
        "            \"pred_adherence\": adherence,\n",
        "            \"pred_context_relevance\": relevance,\n",
        "            \"pred_context_utilization\": utilization,\n",
        "            \"supported\": sample[\"supported\"],\n",
        "            \"relevance\": sample[\"relevance\"],\n",
        "            \"utilization\": sample[\"utilization\"],\n",
        "        }]\n",
        "\n",
        "    print(f\"Completed evaluation on {dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf6219e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "eval_datasets = []\n",
        "for result_dataset in results.values():\n",
        "    eval_datasets += [Dataset.from_list(result_dataset) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce55fe95",
      "metadata": {},
      "source": [
        "3. Evaluate your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23416f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import calculate_metrics\n",
        "\n",
        "all_metrices = []\n",
        "for annotated in eval_datasets:\n",
        "    metrics = calculate_metrics(\n",
        "        annotated,\n",
        "        pred_adherence=\"pred_adherence\",\n",
        "        pred_context_releavance=\"pred_context_relevance\",\n",
        "        pred_context_utilization=\"pred_context_utilization\",\n",
        "        ground_truth_adherence=\"supported\", \n",
        "        ground_truth_context_relevance=\"relevance\",\n",
        "        ground_truth_context_utilization=\"utilization\")\n",
        "    all_metrices += [metrics]\n",
        "\n",
        "all_metrices"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
