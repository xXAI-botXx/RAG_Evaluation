{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f66db6e",
      "metadata": {
        "id": "8f66db6e"
      },
      "source": [
        "# RAG Evaluation with [RAGBench Benchmark](https://huggingface.co/datasets/galileo-ai/ragbench)\n",
        "\n",
        "**Features:**\n",
        "- Multiple datasets to evaluate your model + some evaluation scripts\n",
        "- Used for evaluation of:\n",
        "  - Hallucination Detection\n",
        "  - Context Relevance Detection\n",
        "  - Context Utilization Detection\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**The ugly:**\n",
        "- Does not provide:\n",
        "  - any Guide of how to use it\n",
        "  - requirement dependencies or any setup details\n",
        "  - out-of-the-box models\n",
        "- Old Dependencies (you have to updgrade the *RAGBench code* for modern RAGs)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Content:**\n",
        "- [Python Env](#python-env)\n",
        "- [Example RAG Model](#example-rag-model)\n",
        "  - Retriever: Embedding + Indexing (Database) (+ example data)\n",
        "  - Reranker (we don't use one)\n",
        "  - Generator: Tokenizer + LLM\n",
        "- [Evaluation with RAGBench](#evaluation-with-ragbench)\n",
        "  - 1. Load Datasets\n",
        "  - 2. Evaluate your model\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0428195",
      "metadata": {},
      "source": [
        "### Python Env\n",
        "\n",
        "Install Repository:\n",
        "```bash\n",
        "cd D:\\Informatik\\Projekte\\RAG_Evaluation && D:\n",
        "git clone https://github.com/rungalileo/ragbench.git ./ragbench\n",
        "```\n",
        "Out-Comment foolowing lines in order to does not get in trouble because old-dependencies:\n",
        "- inference.py, line 6\n",
        "- inference.py, line 9 -> remove 'context_relevancy'\n",
        "- trulens_async.py, line 17\n",
        "- trulens_async.py, line 18\n",
        "\n",
        "<br><br>\n",
        "\n",
        "Installation in Anaconda Bash:\n",
        "```bash\n",
        "conda create -n ragbench python=3.12 -y \n",
        "conda activate ragbench\n",
        "\n",
        "pip install ipykernel jupyter notebook ipython\n",
        "\n",
        "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
        "\n",
        "pip install datasets transformers faiss-cpu accelerate prime_printer\n",
        "\n",
        "pip install ragas \n",
        "pip install \"trulens-eval==1.4.0\"\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f509379",
      "metadata": {},
      "source": [
        "### System Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7affa27",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream, resource_exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------- \n",
            "Your Hardware:\n",
            "\n",
            "    ---> General <---\n",
            "Operatingsystem: Windows\n",
            "Version: 10.0.26200\n",
            "Architecture: ('64bit', 'WindowsPE')\n",
            "Processor: AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\n",
            "\n",
            "    ---> GPU <---\n",
            "GPU Name: NVIDIA GeForce RTX 4060\n",
            "VRAM Total: 8188 MB\n",
            "VRAM Used: 1617 MB\n",
            "Utilization: 39.0 %\n",
            "PyTorch Support: True (NVIDIA GeForce RTX 4060)\n",
            "TensorFlow Support: False -> not installed\n",
            "\n",
            "    ---> CPU <---\n",
            "CPU-Name: AMD Ryzen 7 3700X 8-Core Processor\n",
            "CPU Kernels: 8\n",
            "Logical CPU-Kernels: 16\n",
            "CPU-Frequence: 3600 MHz\n",
            "CPU-Utilization: 20.9 %\n",
            "\n",
            "    ---> RAM <---\n",
            "RAM Total: 31 GB\n",
            "RAM Available: 15 GB\n",
            "RAM-Utilization: 50.7 %\n",
            "\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "import prime_printer as prime\n",
        "print(prime.get_hardware())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-CJwn31TDil",
      "metadata": {
        "id": "z-CJwn31TDil"
      },
      "source": [
        "### Example RAG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wGAmEayKTPSe",
      "metadata": {
        "id": "wGAmEayKTPSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "u4Z2il9VTP-L",
      "metadata": {
        "id": "u4Z2il9VTP-L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(30522, 384, padding_idx=0)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", dtype=torch.float16).to(\"cpu\")\n",
        "embedding_model.resize_token_embeddings(len(embedding_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "kVfIVA-OTV2T",
      "metadata": {
        "id": "kVfIVA-OTV2T"
      },
      "outputs": [],
      "source": [
        "example_documents = [\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The Pythagorean theorem describes the relationship between the sides of a right triangle.\",\n",
        "    \"The capital of Germany is Berlin.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1ab44002",
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(model, tokenizer, texts):\n",
        "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)  # CLS Token pooling\n",
        "        # attention_mask = tokens[\"attention_mask\"].unsqueeze(-1)\n",
        "        # embeddings = (outputs.last_hidden_state * attention_mask).sum(dim=1)\n",
        "        # embeddings = embeddings / attention_mask.sum(dim=1)\n",
        "    return embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "QWkCxhy6TZS0",
      "metadata": {
        "id": "QWkCxhy6TZS0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.3853 ,  0.01636,  0.0658 , ...,  0.1343 ,  0.3914 ,  0.2822 ],\n",
              "       [-0.2357 ,  0.3455 , -0.3367 , ...,  0.4631 ,  0.2502 ,  0.0448 ],\n",
              "       [ 0.35   ,  0.08856,  0.197  , ...,  0.1251 ,  0.1675 ,  0.04062]],\n",
              "      shape=(3, 384), dtype=float16)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_embeddings = encode(embedding_model, embedding_tokenizer, example_documents)\n",
        "doc_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gyu2oxxCTzfn",
      "metadata": {
        "id": "Gyu2oxxCTzfn"
      },
      "source": [
        "Build FAISS Index (our \"database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dfdpihG5TxDl",
      "metadata": {
        "id": "dfdpihG5TxDl"
      },
      "outputs": [],
      "source": [
        "dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(doc_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gkGXrFoeT8BP",
      "metadata": {
        "id": "gkGXrFoeT8BP"
      },
      "source": [
        "Load a language model (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6kwAA6cTT8iw",
      "metadata": {
        "id": "6kwAA6cTT8iw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50258, 768)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"gpt2\"  # \"distilgpt2\"\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "generator_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "generator_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             dtype=torch.float16).to('cpu')\n",
        "generator_model.resize_token_embeddings(len(generator_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "32742081",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator_model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KihX3WsSUGDD",
      "metadata": {
        "id": "KihX3WsSUGDD"
      },
      "source": [
        "RAG Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "BBJ5TWimUIAk",
      "metadata": {
        "id": "BBJ5TWimUIAk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rag_answer(query, given_passages, k=2):\n",
        "    # Create prompt + docs embedding\n",
        "    embedded_prompt = encode(embedding_model, embedding_tokenizer, [query])[0]\n",
        "    given_passages_embedded = encode(embedding_model, embedding_tokenizer, given_passages)\n",
        "\n",
        "    # Convert embeddings to float32 numpy arrays\n",
        "    prompt_vec = embedded_prompt.astype(np.float32).reshape(1, -1)\n",
        "    passage_vecs = given_passages_embedded.astype(np.float32)\n",
        "\n",
        "    # Build index\n",
        "    dim = passage_vecs.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(passage_vecs)\n",
        "\n",
        "    # Retrieve top-k docs\n",
        "    distances, indices = index.search(prompt_vec, k)\n",
        "    retrieved = [given_passages[i] for i in indices[0]]\n",
        "\n",
        "    # Build the final prompt for generation\n",
        "    context_text = \"\\n\".join(retrieved)\n",
        "    prompt = (\n",
        "        f\"Use the following context to answer the question.\\n\\n\"\n",
        "        f\"Context: {context_text}\\n\\n\"\n",
        "        f\"Question: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Tokenize final prompt\n",
        "    inputs = generator_tokenizer(prompt, \n",
        "                                 truncation=True,\n",
        "                                 max_length=500,\n",
        "                                 return_tensors=\"pt\")\n",
        "\n",
        "    # Generate\n",
        "    outputs = generator_model.generate(\n",
        "        **inputs,\n",
        "        max_length=500,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=generator_tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode output\n",
        "    answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer[len(prompt):].strip()\n",
        "    return answer, retrieved"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ZcStBgbU722",
      "metadata": {
        "id": "-ZcStBgbU722"
      },
      "source": [
        "Example Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "ELwq7C49U-2-",
      "metadata": {
        "id": "ELwq7C49U-2-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved Docs: ['The Eiffel Tower is located in Paris.', 'The Pythagorean theorem describes the relationship between the sides of a right triangle.']\n",
            "\n",
            "RAG Answer:\n",
            "'The Eiffel Tower is between the Eiffel Tower in Paris and the Aussies in New York City.\n",
            "\n",
            "The Pythagorean theorem describes the relationship between the sides of a right triangle. The Pythagorean theorem is an interesting idea that is not to be confused with the Pythagorean theorem, which states that if two sides are equal in a straight line, the two sides may be equal in a straight line. Thus, the Pythagorean theorem is a theorem by which the triangle must be one of two sides, and this is true if the triangle is equal to one side.\n",
            "\n",
            "The Pythagorean theorem is an interesting idea that is not to be confused with the Pythagorean theorem, which states that if two sides are equal in a straight line, the two sides may be equal in a straight line. Thus, the Pythagorean theorem is a theorem by which the triangle must be one of two sides, and this is true if the triangle is equal to one side. If the first two sides are equal in a straight line, the Pythagorean theorem states that if they are equal in a straight line, they must be equal in a straight line, but if they are equal in a straight line, the Pythagorean theorem states that if they are equal in a straight line, they must be equal in a straight line. Thus, the Pythagorean theorem is a theorem by which the triangle must be one of two sides, and this is true if the triangle is equal to one side. If the first two sides are equal in a straight line, the Pythagorean theorem states that if they are equal in a straight line, they must be equal in a straight line, but if they are equal in a straight line, the Pythagorean theorem states that if they are equal in a straight line, they must be equal in a straight line. Thus, the Pythagorean theorem is a theorem by which the triangle must be one of two sides, and this is true if the triangle is equal to one side.\n",
            "\n",
            "The Pythagorean theorem is an interesting idea that is not to be confused with the Pythagorean theorem, which states that if'\n"
          ]
        }
      ],
      "source": [
        "answer, retrieved_docs = rag_answer(\"Where is the Eiffel Tower located?\", example_documents, k=2)\n",
        "print(f\"Retrieved Docs: {retrieved_docs}\")\n",
        "print(f\"\\nRAG Answer:\\n'{answer}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xeDggQRSVM-P",
      "metadata": {
        "id": "xeDggQRSVM-P"
      },
      "source": [
        "### **Evaluation with RAGBench**\n",
        "\n",
        "Witht he given models it is easy:\n",
        "```bash\n",
        "python run_inference.py --dataset msmarco --model trulens --output results\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877cf20a",
      "metadata": {},
      "source": [
        "1. Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6b5852e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mLoaded 'covidqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'cuad' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'delucionqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'emanual' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'expertqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'finqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'hagrid' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'hotpotqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'msmarco' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'pubmedqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'tatqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0m\u001b[91mLoaded 'techqa' dataset from RAGBench\u001b[0m\n",
            "\u001b[0mColumns in ragbench datasets: {'trulens_groundedness', 'adherence_score', 'documents_sentences', 'response', 'sentence_support_information', 'relevance_score', 'overall_supported_explanation', 'all_relevant_sentence_keys', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'relevance_explanation', 'gpt3_context_relevance', 'id', 'unsupported_response_sentence_keys', 'gpt35_utilization', 'response_sentences', 'generation_model_name', 'question', 'annotating_model_name', 'gpt3_adherence', 'utilization_score', 'dataset_name', 'completeness_score', 'all_utilized_sentence_keys', 'documents'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the full ragbench dataset\n",
        "ragbench = {}\n",
        "columns = set()\n",
        "for dataset in ['covidqa', 'cuad', 'delucionqa', 'emanual', 'expertqa', 'finqa', 'hagrid', 'hotpotqa', 'msmarco', 'pubmedqa', 'tatqa', 'techqa']:\n",
        "  ragbench[dataset] = load_dataset(\"rungalileo/ragbench\", dataset)\n",
        "  prime.awesome_print(f\"Loaded '{dataset}' dataset from RAGBench\", prime.RED)\n",
        "  # columns = columns.union(set(ragbench[dataset]['test'].keys()))\n",
        "  columns = columns.union(set(ragbench[dataset]['test'].column_names))\n",
        "print(f\"Columns in ragbench datasets: {columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "09751b5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = ragbench['delucionqa']['test']\n",
        "small_ds = ds.select([0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd470b4b",
      "metadata": {},
      "source": [
        "2. Run our Model on them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "2962997b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# collect your predictions in a list of dicts\n",
        "data = []\n",
        "\n",
        "for sample in small_ds:\n",
        "    question = sample[\"question\"]\n",
        "    context = sample[\"documents\"]\n",
        "    gold = sample[\"response\"]\n",
        "\n",
        "    pred_answer, pred_contexts = rag_answer(question, given_passages=context, k=3)\n",
        "\n",
        "    data.append({\n",
        "        \"question\": question,\n",
        "        \"gold_answer\": gold,\n",
        "        \"response\": pred_answer,\n",
        "        \"documents\": pred_contexts,\n",
        "    })\n",
        "\n",
        "# convert to Hugging Face Dataset\n",
        "ds_ = Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f581415d",
      "metadata": {},
      "source": [
        "3. Run RAGAS/TruLens annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "7b519821",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path += [\"./ragbench/ragbench\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "8dc94823",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running RAGAS Inference on 1 rows\n"
          ]
        },
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragas_annotate_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m annotated_ds = \u001b[43mragas_annotate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./my_rag_predictions_ragas.jsonl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Informatik\\Projekte\\RAG_Evaluation\\ragbench/ragbench\\inference.py:64\u001b[39m, in \u001b[36mragas_annotate_dataset\u001b[39m\u001b[34m(hf_dataset, output_path)\u001b[39m\n\u001b[32m     61\u001b[39m hf_dataset = hf_dataset.rename_column(RAGBenchFields.CONTEXT, RagasFields.INPUT_CONTEXT)\n\u001b[32m     62\u001b[39m hf_dataset = hf_dataset.rename_column(RAGBenchFields.RESPONSE, RagasFields.INPUT_ANSWER)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m ragas_result = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m ragas_df = ragas_result.to_pandas()\n\u001b[32m     67\u001b[39m annotated_dataset = Dataset.from_pandas(ragas_df)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\ragas\\_analytics.py:277\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    276\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\ragas\\evaluation.py:461\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor, allow_nest_asyncio)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# Default behavior: use nest_asyncio for backward compatibility (Jupyter notebooks)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_async_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\ragas\\async_utils.py:156\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(async_func, allow_nest_asyncio)\u001b[39m\n\u001b[32m    148\u001b[39m     loop_type = \u001b[38;5;28mtype\u001b[39m(loop).\u001b[34m__name__\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    150\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot execute nested async code with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muvloop does not support nested event loop execution. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use asyncio\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms standard event loop in Jupyter environments, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor refactor your code to avoid nested async calls.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\ragas\\evaluation.py:434\u001b[39m, in \u001b[36mevaluate.<locals>._async_wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_async_wrapper\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m aevaluate(\n\u001b[32m    435\u001b[39m         dataset=dataset,\n\u001b[32m    436\u001b[39m         metrics=metrics,\n\u001b[32m    437\u001b[39m         llm=llm,\n\u001b[32m    438\u001b[39m         embeddings=embeddings,\n\u001b[32m    439\u001b[39m         experiment_name=experiment_name,\n\u001b[32m    440\u001b[39m         callbacks=callbacks,\n\u001b[32m    441\u001b[39m         run_config=run_config,\n\u001b[32m    442\u001b[39m         token_usage_parser=token_usage_parser,\n\u001b[32m    443\u001b[39m         raise_exceptions=raise_exceptions,\n\u001b[32m    444\u001b[39m         column_map=column_map,\n\u001b[32m    445\u001b[39m         show_progress=show_progress,\n\u001b[32m    446\u001b[39m         batch_size=batch_size,\n\u001b[32m    447\u001b[39m         _run_id=_run_id,\n\u001b[32m    448\u001b[39m         _pbar=_pbar,\n\u001b[32m    449\u001b[39m         return_executor=return_executor,\n\u001b[32m    450\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\ragas\\evaluation.py:170\u001b[39m, in \u001b[36maevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     llm = llm_factory(\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, client=client)\n\u001b[32m    172\u001b[39m metric.llm = t.cast(t.Optional[BaseRagasLLM], llm)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\ragbench\\Lib\\site-packages\\openai\\_client.py:137\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    135\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "from inference import ragas_annotate_dataset\n",
        "\n",
        "annotated_ds = ragas_annotate_dataset(ds_, output_path=\"./my_rag_predictions_ragas.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008fa747",
      "metadata": {},
      "source": [
        "4. Calculate scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23416f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import calculate_metrics\n",
        "\n",
        "metrics = calculate_metrics(\n",
        "    annotated_ds,\n",
        "    pred_adherence=\"pred_adherence\",\n",
        "    pred_context_utilization=\"pred_context_utilization\"\n",
        ")\n",
        "\n",
        "metrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ragbench",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
