{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f66db6e",
      "metadata": {
        "id": "8f66db6e"
      },
      "source": [
        "# RAG Evaluation with [BERGEN Benchmark](https://github.com/naver/bergen/)\n",
        "\n",
        "- Have out-of-the-box models and datasets\n",
        "- Can add custom models and datasets\n",
        "- Possible to use only partwise models (for example only a custom reranker and the rest is used out-of-the-box)\n",
        "- Need many dependencies and additional code for your custom components\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**The ugly:**\n",
        "- Broken Setup (/Dependencies)\n",
        "  - Tried with Anaconda and Docker\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Content:**\n",
        "- [Python Env](#python-env)\n",
        "- [Example RAG Model](#example-rag-model)\n",
        "  - Retriever: Embedding + Indexing (Database) (+ example data)\n",
        "  - Reranker (we don't use one)\n",
        "  - Generator: Tokenizer + LLM\n",
        "- [Evaluation with BERGEN](#evaluation-with-bergen)\n",
        "  - 1. Defining our Model in BERGEN Repo\n",
        "    - Classes + Configs\n",
        "  - 2. Evaluate your model with Bergen\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0428195",
      "metadata": {},
      "source": [
        "### Python Env\n",
        "\n",
        "Install Repository:\n",
        "```bash\n",
        "git clone https://github.com/naver/bergen.git ./bergen\n",
        "```\n",
        "\n",
        "<br><br>\n",
        "\n",
        "Tried with **Anaconda** and **Python 3.10** & **3.11**\n",
        "\n",
        "Tried with **Docker** and **CUDA 11.2**, **11.8**, **13.0** + **Python 3.9**, **3.10**, **3.12**\n",
        "\n",
        "Tried with **Google Coolab**\n",
        "\n",
        "<span style=\"color:red\">=> All Setups did not work</span>\n",
        "\n",
        "<span style=\"color:red\">*but the procedure still can be shown</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f509379",
      "metadata": {},
      "source": [
        "### System Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7affa27",
      "metadata": {},
      "outputs": [],
      "source": [
        "import prime_printer as prime\n",
        "print(prime.get_hardware())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-CJwn31TDil",
      "metadata": {
        "id": "z-CJwn31TDil"
      },
      "source": [
        "### Example RAG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wGAmEayKTPSe",
      "metadata": {
        "id": "wGAmEayKTPSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u4Z2il9VTP-L",
      "metadata": {
        "id": "u4Z2il9VTP-L"
      },
      "outputs": [],
      "source": [
        "embedding_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", dtype=torch.float16)\n",
        "embedding_model.resize_token_embeddings(len(embedding_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kVfIVA-OTV2T",
      "metadata": {
        "id": "kVfIVA-OTV2T"
      },
      "outputs": [],
      "source": [
        "example_documents = [\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The Pythagorean theorem describes the relationship between the sides of a right triangle.\",\n",
        "    \"The capital of Germany is Berlin.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dfbd23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(model, tokenizer, texts):\n",
        "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QWkCxhy6TZS0",
      "metadata": {
        "id": "QWkCxhy6TZS0"
      },
      "outputs": [],
      "source": [
        "doc_embeddings = encode(embedding_model, embedding_tokenizer, example_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gyu2oxxCTzfn",
      "metadata": {
        "id": "Gyu2oxxCTzfn"
      },
      "source": [
        "Build FAISS Index (our \"database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfdpihG5TxDl",
      "metadata": {
        "id": "dfdpihG5TxDl"
      },
      "outputs": [],
      "source": [
        "dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(doc_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gkGXrFoeT8BP",
      "metadata": {
        "id": "gkGXrFoeT8BP"
      },
      "source": [
        "Load a language model (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6kwAA6cTT8iw",
      "metadata": {
        "id": "6kwAA6cTT8iw"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"  # \"distilgpt2\"\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "generator_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "generator_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             dtype=torch.float16)\n",
        "generator_model.resize_token_embeddings(len(generator_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015a8683",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator_model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KihX3WsSUGDD",
      "metadata": {
        "id": "KihX3WsSUGDD"
      },
      "source": [
        "RAG Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BBJ5TWimUIAk",
      "metadata": {
        "id": "BBJ5TWimUIAk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rag_answer(query, given_passages, k=2):\n",
        "    # Create prompt + docs embedding\n",
        "    embedded_prompt = encode(embedding_model, embedding_tokenizer, [query])[0]\n",
        "    given_passages_embedded = encode(embedding_model, embedding_tokenizer, given_passages)\n",
        "\n",
        "    # Convert embeddings to float32 numpy arrays\n",
        "    prompt_vec = embedded_prompt.astype(np.float32).reshape(1, -1)\n",
        "    passage_vecs = given_passages_embedded.astype(np.float32)\n",
        "\n",
        "    # Build index\n",
        "    dim = passage_vecs.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(passage_vecs)\n",
        "\n",
        "    # Retrieve top-k docs\n",
        "    distances, indices = index.search(prompt_vec, k)\n",
        "    retrieved = [given_passages[i] for i in indices[0]]\n",
        "\n",
        "    # Build the final prompt for generation\n",
        "    context_text = \"\\n\".join(retrieved)\n",
        "    prompt = (\n",
        "        f\"Use the following context to answer the question.\\n\\n\"\n",
        "        f\"Context: {context_text}\\n\\n\"\n",
        "        f\"Question: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Tokenize final prompt\n",
        "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate\n",
        "    outputs = generator_model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode output\n",
        "    answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer[len(prompt):].strip()\n",
        "    return answer, retrieved"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ZcStBgbU722",
      "metadata": {
        "id": "-ZcStBgbU722"
      },
      "source": [
        "Example Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ELwq7C49U-2-",
      "metadata": {
        "id": "ELwq7C49U-2-"
      },
      "outputs": [],
      "source": [
        "answer, retrieved_docs = rag_answer(\"Where is the Eiffel Tower located?\", example_documents, k=2)\n",
        "print(f\"Retrieved Docs: {retrieved_docs}\")\n",
        "print(f\"\\nRAG Answer:\\n'{answer}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xeDggQRSVM-P",
      "metadata": {
        "id": "xeDggQRSVM-P"
      },
      "source": [
        "### **Evaluation with BERGEN**\n",
        "\n",
        "[See documentation](https://github.com/naver/bergen/blob/main/documentation/extensions.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zp4lqto8ODCG",
      "metadata": {
        "id": "zp4lqto8ODCG"
      },
      "source": [
        "### 1. Defining our Model in BERGEN Repo\n",
        "\n",
        "You can add a custom:\n",
        "- Retriever\n",
        "- Reranker\n",
        "- Generator\n",
        "- Dataset\n",
        "\n",
        "Or you choose a out-of-the-box choice.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Retriever**\n",
        "- inherit from `models.retrievers.retriever.Retriever`\n",
        "- needed methods:\n",
        "  - `collate_fn(self, batch, query_or_doc=None)`\n",
        "  - `__call__(self, kwargs)`\n",
        "  - `similarity_fn(self, q_embs, doc_embs)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BM71B0A1VPaM",
      "metadata": {
        "id": "BM71B0A1VPaM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "new_retriever = \"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from models.retrievers.retriever import Retriever\n",
        "\n",
        "class NewRetriever(Retriever):\n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                          device_map=\"auto\",\n",
        "                                                          dtype=torch.float16)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def encode(self, texts):\n",
        "        tokens = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**tokens)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "    def collate_fn(self, batch, query_or_doc=None):\n",
        "        if isinstance(batch[0], dict):\n",
        "            return [sample[\"content\"] for sample in batch]\n",
        "        return batch\n",
        "\n",
        "    def __call__(self, kwargs):\n",
        "        texts = kwargs[\"content\"]\n",
        "        emb = self.encode(texts)\n",
        "        return {\"embeddings\": emb, \"raw_texts\": texts}\n",
        "\n",
        "    def similarity_fn(self, q_embs, doc_embs):\n",
        "        return torch.matmul(q_embs, doc_embs.T)\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/models/retrievers/\", exist_ok=True)\n",
        "with open(\"./bergen/models/retrievers/new_retriever.py\", \"w\") as f:\n",
        "  f.write(new_retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordqZnyvVmZW",
      "metadata": {
        "id": "ordqZnyvVmZW"
      },
      "source": [
        "Add config yaml to `config/retriever`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N8Da0RwYVmsl",
      "metadata": {
        "id": "N8Da0RwYVmsl"
      },
      "outputs": [],
      "source": [
        "new_retriever_config = \"\"\"\n",
        "init_args:\n",
        "  _target_: models.retrievers.new_retriever.NewRetriever\n",
        "  model_name: \"new_retriever\"\n",
        "batch_size: 1024\n",
        "batch_size_sim: 256\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/retriever/\", exist_ok=True)\n",
        "with open(\"./bergen/config/retriever/new_retriever.yaml\", \"w\") as f:\n",
        "  f.write(new_retriever_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2EdXGaQTRO1k",
      "metadata": {
        "id": "2EdXGaQTRO1k"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "**Reranker**\n",
        "- inherit from `models.rerankers.reranker.Reranker`\n",
        "- needed methods:\n",
        "  - `collate_fn(self, batch, query_or_doc=None)`\n",
        "  - `__call__(self, kwargs)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gTjNyHMaNE5O",
      "metadata": {
        "id": "gTjNyHMaNE5O"
      },
      "outputs": [],
      "source": [
        "new_reranker = \"\"\"\n",
        "from models.rerankers.reranker import Reranker\n",
        "\n",
        "class NewReranker(Reranker):\n",
        "    def __init__(self, model_name=None):\n",
        "        self.model_name = 'no_reranker'\n",
        "\n",
        "    def collate_fn(self, batch, query_or_doc=None):\n",
        "        return batch\n",
        "\n",
        "    def __call__(self, kwargs):\n",
        "        return kwargs\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/models/rerankers/\", exist_ok=True)\n",
        "with open(\"./bergen/models/rerankers/new_reranker.py\", \"w\") as f:\n",
        "  f.write(new_reranker)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wV2Q7ZaxVKOY",
      "metadata": {
        "id": "wV2Q7ZaxVKOY"
      },
      "source": [
        "Add config yaml to `config/reranker`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4klwoUW7VJoM",
      "metadata": {
        "id": "4klwoUW7VJoM"
      },
      "outputs": [],
      "source": [
        "new_reranker_config = \"\"\"\n",
        "init_args:\n",
        "  _target_: models.rerankers.new_reranker.NewReranker\n",
        "  model_name: \"new_reranker\"\n",
        "batch_size: 2048\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/reranker/\", exist_ok=True)\n",
        "with open(\"./bergen/config/reranker/new_reranker.yaml\", \"w\") as f:\n",
        "  f.write(new_reranker_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zr4Mpy1ERcuA",
      "metadata": {
        "id": "Zr4Mpy1ERcuA"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "**Generator**\n",
        "- inherit from `models.generators.generator.Generator`\n",
        "- needed methods:\n",
        "  - `collate_fn(self, inp)`\n",
        "  - `generate(self, inp)`\n",
        "  - `prediction_step(self, model, model_input, label_ids=None)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w3DHROJ4NpAS",
      "metadata": {
        "id": "w3DHROJ4NpAS"
      },
      "outputs": [],
      "source": [
        "new_generator = \"\"\"\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from models.generators.generator import Generator\n",
        "\n",
        "class NewGenerator(Generator):\n",
        "    def __init__(self, model_name=\"gpt2\"):\n",
        "        self.model_name = \"gpt-2\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                          device_map=\"auto\",\n",
        "                                                          dtype=torch.float16)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def collate_fn(self, inp):\n",
        "        return self.tokenizer(\n",
        "            inp,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def generate(self, inp):\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inp[\"input_ids\"],\n",
        "            attention_mask=inp[\"attention_mask\"],\n",
        "            max_length=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    def prediction_step(self, model, model_input, label_ids=None):\n",
        "        output = model(**model_input, labels=label_ids)\n",
        "        return output.logits, output.loss\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/models/generators/\", exist_ok=True)\n",
        "with open(\"./bergen/models/generators/new_generator.py\", \"w\") as f:\n",
        "  f.write(new_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "InIxYo18U0ed",
      "metadata": {
        "id": "InIxYo18U0ed"
      },
      "source": [
        "Add config yaml to `config/generators`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w1MetpWuUoLg",
      "metadata": {
        "id": "w1MetpWuUoLg"
      },
      "outputs": [],
      "source": [
        "new_generator_config = \"\"\"\n",
        "init_args:\n",
        "  _target_: models.generators.new_generator.NewGenerator\n",
        "  model_name: \"new_generator\"\n",
        "  max_new_tokens: 128\n",
        "batch_size: 32\n",
        "max_inp_length: null\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/generator/\", exist_ok=True)\n",
        "with open(\"./bergen/config/generator/new_generator.yaml\", \"w\") as f:\n",
        "  f.write(new_generator_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_3KPLZAwSU7M",
      "metadata": {
        "id": "_3KPLZAwSU7M"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "Other:\n",
        "\n",
        "**Dataset**\n",
        "- inherit from `modules.dataset_processor.Processor`\n",
        "- needed methods:\n",
        "  - `__init__(self, *args, **kwargs)`\n",
        "  - `process(self)`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MQxsdxhSWGhp",
      "metadata": {
        "id": "MQxsdxhSWGhp"
      },
      "source": [
        "Add config yaml to `config/generators`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MeK_XJNtWBzl",
      "metadata": {
        "id": "MeK_XJNtWBzl"
      },
      "outputs": [],
      "source": [
        "new_dataset_config = \"\"\"\n",
        "test:\n",
        "    doc: null\n",
        "    query: null\n",
        "dev:\n",
        "  doc:\n",
        "    init_args:\n",
        "    _target_: modules.dataset_processor.NewDataset\n",
        "    split: \"full\"\n",
        "query:\n",
        "  init_args:\n",
        "    _target_: modules.dataset_processor.KILTNQProcessor\n",
        "    split: \"validation\"\n",
        "train:\n",
        "    doc: null\n",
        "    query: null\n",
        "\"\"\"\n",
        "\n",
        "with open(\"./bergen/config/dataset/new_config.yaml\", \"w\") as f:\n",
        "  f.write(new_dataset_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9sR-2091WZfZ",
      "metadata": {
        "id": "9sR-2091WZfZ"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "\n",
        "**Prompt**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tXV9iLphWjdB",
      "metadata": {
        "id": "tXV9iLphWjdB"
      },
      "outputs": [],
      "source": [
        "new_prompt_config = \"\"\"\n",
        "system: \"You are a helpful assistant. Your task is to extract relevant information from the provided documents and to answer questions accordingly.\"\n",
        "user: f\"Background:\\ {docs}\\n\\nQuestion:\\ {question}\\nAnswer:\"\n",
        "system_without_docs: \"You are a helpful assistant.\"\n",
        "user_without_docs: f\"Question:\\ {question}\\nAnswer:\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"./bergen/config/prompt/new_prompt.yaml\", \"w\") as f:\n",
        "  f.write(new_prompt_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mHpE1fm-O9qh",
      "metadata": {
        "id": "mHpE1fm-O9qh"
      },
      "source": [
        "### 2. Evaluate your model with Bergen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p-2mBGBIPAO6",
      "metadata": {
        "id": "p-2mBGBIPAO6"
      },
      "outputs": [],
      "source": [
        "!python ./bergen/bergen.py retriever='new_retriever' \\\n",
        "                           reranker='new_reranker' \\\n",
        "                           generator='new_generator' \\\n",
        "                           dataset='kilt_hotpotqa'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bergen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
