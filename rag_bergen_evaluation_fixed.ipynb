{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f66db6e",
      "metadata": {
        "id": "8f66db6e"
      },
      "source": [
        "# RAG Evaluation with [BERGEN Benchmark](https://github.com/naver/bergen/)\n",
        "\n",
        "Consist of out-of-the-box models and datasets. Can add custom models and datasets. Possible to use only partwise models (for example only a custom reranker and the rest is used out-of-the-box). Need many dependencies and additional code to use custom components.\n",
        "\n",
        "> Warning: Installation and Dependencies are complex. We will use the sandbox environment's pre-installed Python 3.11 and `pip` for a clean setup.\n",
        "\n",
        "- [Python Env Setup](#python-env-setup)\n",
        "- [Example RAG Model](#example-rag-model)\n",
        "- [Evaluation with BERGEN](#evaluation-with-bergen)\n",
        "  - 1. Defining our Model in BERGEN Repo\n",
        "    - Classes + Configs\n",
        "  - 2. Evaluate your model with Bergen\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0428195",
      "metadata": {},
      "source": [
        "### Python Env Setup\n",
        "\n",
        "**Note:** The following steps assume you are running this notebook in the `/home/ubuntu/` directory, where the `bergen` repository has been cloned.\n",
        "\n",
        "1. **Clone Repository (Already done in the sandbox):**\n",
        "    ```bash\n",
        "    git clone https://github.com/naver/bergen.git ./bergen\n",
        "    ```\n",
        "\n",
        "2. **Create Anacona Env:**\n",
        "    ```bash\n",
        "    conda create -n bergen-fixed python=3.11 -y\n",
        "    conda activate bergen-fixed\n",
        "    ```\n",
        "\n",
        "3. **Install Dependencies (Already done in the sandbox):**\n",
        "The following command installs the necessary packages, including `torch`, `transformers`, `faiss-cpu`, `datasets`, `hydra-core`, `omegaconf`, `pytrec_eval`, and `torchinfo`.\n",
        "    ```bash\n",
        "    pip3 install torch transformers faiss-cpu datasets hydra-core omegaconf pytrec_eval torchinfo jupyter ipykernel\n",
        "    ```\n",
        "\n",
        "**Please ensure you select the `Python 3.11 (bergen)` kernel for this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f7affa27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Nov 30 11:41:21 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 581.29                 Driver Version: 581.29         CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4060      WDDM  |   00000000:26:00.0  On |                  N/A |\n",
            "|  0%   36C    P8            N/A  /  115W |    2745MiB /   8188MiB |      7%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi | sed \"/Processes/,$d\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-CJwn31TDil",
      "metadata": {
        "id": "z-CJwn31TDil"
      },
      "source": [
        "### Example RAG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "wGAmEayKTPSe",
      "metadata": {
        "id": "wGAmEayKTPSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import faiss\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "u4Z2il9VTP-L",
      "metadata": {
        "id": "u4Z2il9VTP-L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(30522, 384, padding_idx=0)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# Use torch.float32 for CPU/general compatibility, as float16 might cause issues on CPU or without a dedicated GPU.\n",
        "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", dtype=torch.float32)\n",
        "embedding_model.resize_token_embeddings(len(embedding_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c8477e30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Tokenizer loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Removed tokenizer summary as it is not a torch.nn.Module\n",
        "print(\"Embedding Tokenizer loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "59cfee6b",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\torchinfo\\torchinfo.py:301\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    298\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m             \u001b[38;5;66;03m# Should not reach this point, since process_input_data ensures\u001b[39;00m\n\u001b[32m    300\u001b[39m             \u001b[38;5;66;03m# x is either a list, tuple, or dict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnknown input type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[31mValueError\u001b[39m: Unknown input type",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dummy_input = embedding_tokenizer(\u001b[33m\"\u001b[39m\u001b[33mThis is a test sentence.\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass the actual input data\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m validate_user_params(\n\u001b[32m    217\u001b[39m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m x, correct_input_size = process_input(\n\u001b[32m    221\u001b[39m     input_data, input_size, batch_dim, device, dtypes\n\u001b[32m    222\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m summary_list = \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m formatting = FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[32m    227\u001b[39m results = ModelStatistics(\n\u001b[32m    228\u001b[39m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[32m    229\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     executed_layers = [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer.executed]\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
            "\u001b[31mRuntimeError\u001b[39m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
          ]
        }
      ],
      "source": [
        "dummy_input = embedding_tokenizer(\"This is a test sentence.\", return_tensors=\"pt\")\n",
        "summary(\n",
        "    embedding_model,\n",
        "    input_data=dummy_input, # Pass the actual input data\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "kVfIVA-OTV2T",
      "metadata": {
        "id": "kVfIVA-OTV2T"
      },
      "outputs": [],
      "source": [
        "example_documents = [\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The Pythagorean theorem describes the relationship between the sides of a right triangle.\",\n",
        "    \"The capital of Germany is Berlin.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6dfbd23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(model, tokenizer, texts):\n",
        "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        # Mean pooling of the last hidden state for sentence embedding\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "QWkCxhy6TZS0",
      "metadata": {
        "id": "QWkCxhy6TZS0"
      },
      "outputs": [],
      "source": [
        "doc_embeddings = encode(embedding_model, embedding_tokenizer, example_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gyu2oxxCTzfn",
      "metadata": {
        "id": "Gyu2oxxCTzfn"
      },
      "source": [
        "Build FAISS Index (our \"database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dfdpihG5TxDl",
      "metadata": {
        "id": "dfdpihG5TxDl"
      },
      "outputs": [],
      "source": [
        "dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(doc_embeddings.astype('float32')) # FAISS requires float32"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gkGXrFoeT8BP",
      "metadata": {
        "id": "gkGXrFoeT8BP"
      },
      "source": [
        "Load a language model (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6kwAA6cTT8iw",
      "metadata": {
        "id": "6kwAA6cTT8iw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Embedding(50258, 768)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"gpt2\"  # \"distilgpt2\"\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "generator_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# Use torch.float32 for CPU/general compatibility\n",
        "generator_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             #device_map=\"auto\",\n",
        "                                             dtype=torch.float32)\n",
        "generator_model.resize_token_embeddings(len(generator_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2ce32016",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator Tokenizer loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Removed tokenizer summary as it is not a torch.nn.Module\n",
        "print(\"Generator Tokenizer loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2f653e37",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\torchinfo\\torchinfo.py:301\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    298\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m             \u001b[38;5;66;03m# Should not reach this point, since process_input_data ensures\u001b[39;00m\n\u001b[32m    300\u001b[39m             \u001b[38;5;66;03m# x is either a list, tuple, or dict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnknown input type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[31mValueError\u001b[39m: Unknown input type",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dummy_input = generator_tokenizer(\u001b[33m\"\u001b[39m\u001b[33mThis is a test sentence.\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass the actual input data\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m validate_user_params(\n\u001b[32m    217\u001b[39m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m x, correct_input_size = process_input(\n\u001b[32m    221\u001b[39m     input_data, input_size, batch_dim, device, dtypes\n\u001b[32m    222\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m summary_list = \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m formatting = FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[32m    227\u001b[39m results = ModelStatistics(\n\u001b[32m    228\u001b[39m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[32m    229\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     executed_layers = [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer.executed]\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
            "\u001b[31mRuntimeError\u001b[39m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
          ]
        }
      ],
      "source": [
        "dummy_input = generator_tokenizer(\"This is a test sentence.\", return_tensors=\"pt\")\n",
        "summary(\n",
        "    generator_model,\n",
        "    input_data=dummy_input, # Pass the actual input data\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "015a8683",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator model device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(f\"Generator model device: {generator_model.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KihX3WsSUGDD",
      "metadata": {
        "id": "KihX3WsSUGDD"
      },
      "source": [
        "RAG Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "BBJ5TWimUIAk",
      "metadata": {
        "id": "BBJ5TWimUIAk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Where is the famous tower in Paris?\n",
            "Retrieved Documents: ['The Eiffel Tower is located in Paris.', 'The Pythagorean theorem describes the relationship between the sides of a right triangle.']\n",
            "Generated Answer: It is in the middle of the city.\n",
            "\n",
            "The Pythagorean theorem is one of the most famous facts about the architecture of the United States. The Eiffel Tower is one of the most famous buildings in the world and is famous for\n"
          ]
        }
      ],
      "source": [
        "def rag_answer(query, given_passages, k=2):\n",
        "    # Create prompt + docs embedding\n",
        "    embedded_prompt = encode(embedding_model, embedding_tokenizer, [query])[0]\n",
        "    given_passages_embedded = encode(embedding_model, embedding_tokenizer, given_passages)\n",
        "\n",
        "    # Convert embeddings to float32 numpy arrays\n",
        "    prompt_vec = embedded_prompt.astype(np.float32).reshape(1, -1)\n",
        "    passage_vecs = given_passages_embedded.astype(np.float32)\n",
        "\n",
        "    # Build index\n",
        "    dim = passage_vecs.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(passage_vecs)\n",
        "\n",
        "    # Retrieve top-k docs\n",
        "    distances, indices = index.search(prompt_vec, k)\n",
        "    retrieved = [given_passages[i] for i in indices[0]]\n",
        "\n",
        "    # Build the final prompt for generation\n",
        "    context_text = \"\\n\".join(retrieved)\n",
        "    prompt = (\n",
        "        f\"Use the following context to answer the question.\\n\\n\"\n",
        "        f\"Context:\\n{context_text}\\n\\n\"\n",
        "        f\"Question: {query}\\n\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    # Generate answer\n",
        "    input_ids = generator_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    # Move to model device if not already there\n",
        "    input_ids = input_ids.to(generator_model.device)\n",
        "\n",
        "    # Set pad_token_id to eos_token_id for open-ended generation if it's not set\n",
        "    if generator_tokenizer.pad_token_id is None:\n",
        "        generator_tokenizer.pad_token_id = generator_tokenizer.eos_token_id\n",
        "\n",
        "    output = generator_model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids.shape[-1] + 50, # Generate up to 50 new tokens\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=generator_tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = generator_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Remove the prompt part from the generated text\n",
        "    answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "    return answer, retrieved\n",
        "\n",
        "query = \"Where is the famous tower in Paris?\"\n",
        "answer, retrieved_docs = rag_answer(query, example_documents)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved Documents: {retrieved_docs}\")\n",
        "print(f\"Generated Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q4Y1x8Y5T8-L",
      "metadata": {
        "id": "q4Y1x8Y5T8-L"
      },
      "source": [
        "---\n",
        "\n",
        "## Evaluation with BERGEN\n",
        "\n",
        "### 1. Defining our Model in BERGEN Repo\n",
        "\n",
        "**Note:** We will create the necessary files in the cloned `bergen` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q4Y1x8Y5T8-L",
      "metadata": {
        "id": "Q4Y1x8Y5T8-L"
      },
      "source": [
        "**Retriever**\n",
        "- inherit from `models.retrievers.retriever.Retriever`\n",
        "- needed methods:\n",
        "  - `collate_fn(self, batch, query_or_doc=None)`\n",
        "  - `__call__(self, kwargs)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2tvrCaJBV8I6gabDLa4YCL",
      "metadata": {
        "id": "2tvrCaJBV8I6gabDLa4YCL"
      },
      "outputs": [],
      "source": [
        "new_retriever = \"\"\"\n",
        "from models.retrievers.retriever import Retriever\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "class NewRetriever(Retriever):\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
        "        # Call super().__init__ if the base class requires it, but for a simple custom retriever, it might be optional.\n",
        "        # super().__init__(**kwargs)\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        # Use float32 for compatibility\n",
        "        self.model = AutoModel.from_pretrained(model_name, dtype=torch.float32)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def collate_fn(self, batch, query_or_doc=None):\n",
        "        # This is a simplified collate_fn for demonstration\n",
        "        return self.tokenizer(\n",
        "            batch,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def __call__(self, kwargs):\n",
        "        # This is a simplified __call__ for demonstration\n",
        "        tokens = kwargs['input_ids']\n",
        "        attention_mask = kwargs['attention_mask']\n",
        "        \n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=tokens, attention_mask=attention_mask)\n",
        "            # Mean pooling\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        \n",
        "        # BERGEN expects a dictionary with 'embedding' key\n",
        "        return {'embedding': embeddings.cpu().numpy()}\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/models/retrievers/\", exist_ok=True)\n",
        "with open(\"./bergen/models/retrievers/new_retriever.py\", \"w\") as f:\n",
        "  f.write(new_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "Q4Y1x8Y5T8-L-config",
      "metadata": {
        "id": "Q4Y1x8Y5T8-L-config"
      },
      "outputs": [],
      "source": [
        "new_retriever_config = \"\"\"\n",
        "init_args:\n",
        "  _target_: models.retrievers.new_retriever.NewRetriever\n",
        "  model_name: \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "batch_size: 2048\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/retriever/\", exist_ok=True)\n",
        "with open(\"./bergen/config/retriever/new_retriever.yaml\", \"w\") as f:\n",
        "  f.write(new_retriever_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2EdXGaQTRO1k",
      "metadata": {
        "id": "2EdXGaQTRO1k"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "**Reranker**\n",
        "- inherit from `models.rerankers.reranker.Reranker`\n",
        "- needed methods:\n",
        "  - `collate_fn(self, batch, query_or_doc=None)`\n",
        "  - `__call__(self, kwargs)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "gTjNyHMaNE5O",
      "metadata": {
        "id": "gTjNyHMaNE5O"
      },
      "outputs": [],
      "source": [
        "new_reranker = \"\"\"\n",
        "from models.rerankers.reranker import Reranker\n",
        "\n",
        "class NewReranker(Reranker):\n",
        "    def __init__(self, model_name=None):\n",
        "        # BERGEN expects model_name to be set\n",
        "        self.model_name = 'no_reranker'\n",
        "\n",
        "    def collate_fn(self, batch, query_or_doc=None):\n",
        "        # No-op collate function\n",
        "        return batch\n",
        "\n",
        "    def __call__(self, kwargs):\n",
        "        # No-op reranker, returns the input kwargs as is\n",
        "        return kwargs\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/models/rerankers/\", exist_ok=True)\n",
        "with open(\"./bergen/models/rerankers/new_reranker.py\", \"w\") as f:\n",
        "  f.write(new_reranker)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wV2Q7ZaxVKOY",
      "metadata": {},
      "source": [
        "Add config yaml to `config/reranker`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4klwoUW7VJoM",
      "metadata": {
        "id": "4klwoUW7VJoM"
      },
      "outputs": [],
      "source": [
        "new_reranker_config = \"\"\"\n",
        "init_args:\n",
        "  _target_: models.rerankers.new_reranker.NewReranker\n",
        "  model_name: \"new_reranker\"\n",
        "batch_size: 2048\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/reranker/\", exist_ok=True)\n",
        "with open(\"./bergen/config/reranker/new_reranker.yaml\", \"w\") as f:\n",
        "  f.write(new_reranker_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zr4Mpy1ERcuA",
      "metadata": {
        "id": "Zr4Mpy1ERcuA"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "**Generator**\n",
        "- inherit from `models.generators.generator.Generator`\n",
        "- needed methods:\n",
        "  - `collate_fn(self, inp)`\n",
        "  - `generate(self, inp)`\n",
        "  - `prediction_step(self, model, model_input, label_ids=None)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "w3DHROJ4NpAS",
      "metadata": {
        "id": "w3DHROJ4NpAS"
      },
      "outputs": [],
      "source": [
        "new_generator = \"\"\"\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from models.generators.generator import Generator\n",
        "\n",
        "class NewGenerator(Generator):\n",
        "    def __init__(self, model_name=\"gpt2\", max_new_tokens=128):\n",
        "        self.model_name = model_name\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        # Use float32 for compatibility\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                          # device_map=\"auto\",\n",
        "                                                          dtype=torch.float32)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        # Set pad_token_id for generation\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def collate_fn(self, inp):\n",
        "        # inp is a list of strings (prompts)\n",
        "        return self.tokenizer(\n",
        "            inp,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            pad_to_multiple_of=8 # Optimization for some hardware\n",
        "        )\n",
        "\n",
        "    def generate(self, inp):\n",
        "        # inp is the output of collate_fn (a dict of tensors)\n",
        "        # Move tensors to model device\n",
        "        input_ids = inp[\"input_ids\"].to(self.model.device)\n",
        "        attention_mask = inp[\"attention_mask\"].to(self.model.device)\n",
        "        \n",
        "        outputs = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=self.max_new_tokens, # Use config value\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        \n",
        "        # Decode only the newly generated part\n",
        "        decoded_outputs = []\n",
        "        for i, output in enumerate(outputs):\n",
        "            # Slice to get only the generated tokens (after the input prompt)\n",
        "            generated_tokens = output[input_ids.shape[1]:]\n",
        "            decoded_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_outputs.append(decoded_text.strip())\n",
        "            \n",
        "        return decoded_outputs\n",
        "\n",
        "    def prediction_step(self, model, model_input, label_ids=None):\n",
        "        # This method is typically for training/validation, but required by the base class.\n",
        "        # We can simplify it for a basic evaluation setup.\n",
        "        output = model(**model_input, labels=label_ids)\n",
        "        return output.logits, output.loss\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/models/generators/\", exist_ok=True)\n",
        "with open(\"./bergen/models/generators/new_generator.py\", \"w\") as f:\n",
        "  f.write(new_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "InIxYo18U0ed",
      "metadata": {
        "id": "InIxYo18U0ed"
      },
      "source": [
        "Add config yaml to `config/generators`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "w1MetpWuUoLg",
      "metadata": {
        "id": "w1MetpWuUoLg"
      },
      "outputs": [],
      "source": [
        "new_generator_config = \"\"\"\n",
        "init_args:\n",
        "  _target_: models.generators.new_generator.NewGenerator\n",
        "  model_name: \"gpt2\"\n",
        "  max_new_tokens: 128\n",
        "batch_size: 32\n",
        "max_inp_length: null\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/generator/\", exist_ok=True)\n",
        "with open(\"./bergen/config/generator/new_generator.yaml\", \"w\") as f:\n",
        "  f.write(new_generator_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_3KPLZAwSU7M",
      "metadata": {
        "id": "_3KPLZAwSU7M"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "**Dataset**\n",
        "- inherit from `modules.dataset_processor.Processor`\n",
        "- needed methods:\n",
        "  - `__init__(self, *args, **kwargs)`\n",
        "  - `process(self)`\n",
        "\n",
        "**Note:** Since you didn't provide the `NewDataset` class implementation, I will assume you intend to use a standard BERGEN dataset like `kilt_hotpotqa` for the final evaluation, but I will keep the dataset config creation for completeness, using a placeholder target that you can replace with your custom dataset class later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "MeK_XJNtWBzl",
      "metadata": {
        "id": "MeK_XJNtWBzl"
      },
      "outputs": [],
      "source": [
        "new_dataset_config = \"\"\"\n",
        "test:\n",
        "    doc: null\n",
        "    query: null\n",
        "dev:\n",
        "  doc:\n",
        "    init_args:\n",
        "      _target_: modules.dataset_processor.KILTNQProcessor # Placeholder, replace with your custom class if needed\n",
        "      split: \"validation\"\n",
        "  query:\n",
        "    init_args:\n",
        "      _target_: modules.dataset_processor.KILTNQProcessor # Placeholder, replace with your custom class if needed\n",
        "      split: \"validation\"\n",
        "train:\n",
        "    doc: null\n",
        "    query: null\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/dataset/\", exist_ok=True)\n",
        "with open(\"./bergen/config/dataset/new_config.yaml\", \"w\") as f:\n",
        "  f.write(new_dataset_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9sR-2091WZfZ",
      "metadata": {
        "id": "9sR-2091WZfZ"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "\n",
        "**Prompt**\n",
        "This config defines the prompt template for the generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "tXV9iLphWjdB",
      "metadata": {
        "id": "tXV9iLphWjdB"
      },
      "outputs": [],
      "source": [
        "new_prompt_config = \"\"\"\n",
        "system: \"You are a helpful assistant. Your task is to extract relevant information from the provided documents and to answer questions accordingly.\"\n",
        "user: \"Background:\\n{docs}\\n\\nQuestion:\\n{question}\\nAnswer:\"\n",
        "system_without_docs: \"You are a helpful assistant.\"\n",
        "user_without_docs: \"Question:\\n{question}\\nAnswer:\"\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"./bergen/config/prompt/\", exist_ok=True)\n",
        "with open(\"./bergen/config/prompt/new_prompt.yaml\", \"w\") as f:\n",
        "  f.write(new_prompt_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mHpE1fm-O9qh",
      "metadata": {
        "id": "mHpE1fm-O9qh"
      },
      "source": [
        "### 2. Evaluate your model with Bergen\n",
        "\n",
        "Run the BERGEN evaluation script using the custom components and the `kilt_hotpotqa` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "p-2mBGBIPAO6",
      "metadata": {
        "id": "p-2mBGBIPAO6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfinished experiment_folder: experiments/tmp_9f14019e8482790a\n",
            "experiment_folder experiments/9f14019e8482790a\n",
            "run_name: null\n",
            "dataset_folder: datasets/\n",
            "index_folder: indexes/\n",
            "runs_folder: runs/\n",
            "generated_query_folder: generated_queries/\n",
            "processed_context_folder: processed_contexts/\n",
            "experiments_folder: experiments/\n",
            "retrieve_top_k: 50\n",
            "rerank_top_k: 50\n",
            "generation_top_k: 5\n",
            "pyserini_num_threads: 20\n",
            "processing_num_proc: 40\n",
            "retriever:\n",
            "  init_args:\n",
            "    _target_: models.retrievers.new_retriever.NewRetriever\n",
            "    model_name: sentence-transformers/all-MiniLM-L6-v2\n",
            "  batch_size: 2048\n",
            "reranker:\n",
            "  init_args:\n",
            "    _target_: models.rerankers.new_reranker.NewReranker\n",
            "    model_name: new_reranker\n",
            "  batch_size: 2048\n",
            "generator:\n",
            "  init_args:\n",
            "    _target_: models.generators.new_generator.NewGenerator\n",
            "    model_name: gpt2\n",
            "    max_new_tokens: 128\n",
            "  batch_size: 32\n",
            "  max_inp_length: null\n",
            "dataset:\n",
            "  train:\n",
            "    doc:\n",
            "      init_args:\n",
            "        _target_: modules.dataset_processor.KILT100w\n",
            "        split: full\n",
            "    query:\n",
            "      init_args:\n",
            "        _target_: modules.processors.kilt_dataset_processor.KILTNQ\n",
            "        split: train\n",
            "  dev:\n",
            "    doc:\n",
            "      init_args:\n",
            "        _target_: modules.dataset_processor.KILT100w\n",
            "        split: full\n",
            "    query:\n",
            "      init_args:\n",
            "        _target_: modules.processors.kilt_dataset_processor.KILTNQ\n",
            "        split: validation\n",
            "  test:\n",
            "    doc: null\n",
            "    query: null\n",
            "prompt:\n",
            "  system: You are a helpful assistant. Your task is to extract relevant information\n",
            "    from the provided documents and to answer questions accordingly.\n",
            "  user: 'Background: {docs}\n",
            "\n",
            "    Question: {question} Answer:'\n",
            "  system_without_docs: You are a helpful assistant.\n",
            "  user_without_docs: 'Question: {question} Answer:'\n",
            "\n",
            "Processing dataset kilt-100w in full split \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error executing job with overrides: ['retriever=new_retriever', 'reranker=new_reranker', 'generator=new_generator', 'dataset=kilt_nq', 'prompt=new_prompt']\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\Informatik\\Projekte\\RAG_Evaluation\\bergen\\bergen.py\", line 18, in main\n",
            "    rag = RAG(**config, config=config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Informatik\\Projekte\\RAG_Evaluation\\bergen\\modules\\rag.py\", line 160, in __init__\n",
            "    self.datasets = ProcessDatasets.process(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Informatik\\Projekte\\RAG_Evaluation\\bergen\\modules\\dataset_processor.py\", line 674, in process\n",
            "    dataset = processor.get_dataset()\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Informatik\\Projekte\\RAG_Evaluation\\bergen\\modules\\dataset_processor.py\", line 92, in get_dataset\n",
            "    dataset = self.process()\n",
            "              ^^^^^^^^^^^^^^\n",
            "  File \"d:\\Informatik\\Projekte\\RAG_Evaluation\\bergen\\modules\\dataset_processor.py\", line 305, in process\n",
            "    dataset = datasets.load_dataset(hf_name, num_proc=self.num_proc)[self.split]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\datasets\\load.py\", line 1397, in load_dataset\n",
            "    builder_instance = load_dataset_builder(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\datasets\\load.py\", line 1137, in load_dataset_builder\n",
            "    dataset_module = dataset_module_factory(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\datasets\\load.py\", line 1036, in dataset_module_factory\n",
            "    raise e1 from None\n",
            "  File \"d:\\Programme\\Anaconda\\envs\\bergen-fixed\\Lib\\site-packages\\datasets\\load.py\", line 994, in dataset_module_factory\n",
            "    raise RuntimeError(f\"Dataset scripts are no longer supported, but found {filename}\")\n",
            "RuntimeError: Dataset scripts are no longer supported, but found kilt_wikipedia.py\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ]
        }
      ],
      "source": [
        "# Use the custom configs we just created\n",
        "!python ./bergen/bergen.py retriever=new_retriever \\\n",
        "                           reranker=new_reranker \\\n",
        "                           generator=new_generator \\\n",
        "                           dataset=kilt_nq \\\n",
        "                           prompt=new_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "38472d60",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pyarrow version: 22.0.0\n"
          ]
        }
      ],
      "source": [
        "import pyarrow\n",
        "print(f\"pyarrow version: {pyarrow.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c9b5d7a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-evaluation analysis commands are ready to run after the main BERGEN evaluation.\n"
          ]
        }
      ],
      "source": [
        "# These commands are for post-evaluation analysis, which you can run after the main evaluation is complete.\n",
        "# !python evaluate.py --experiments_folder experiments/ --llm_batch_size 16 --split 'dev' --llm vllm_SOLAR-107B\n",
        "# !python print_results.py --folder experiments/ --format=tiny\n",
        "print(\"Post-evaluation analysis commands are ready to run after the main BERGEN evaluation.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bergen-fixed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
